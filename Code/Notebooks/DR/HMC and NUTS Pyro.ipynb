{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "from torch.nn import Parameter\n",
    "\n",
    "import pyro\n",
    "from pyro.contrib.gp.util import Parameterized\n",
    "import pyro.distributions as dist\n",
    "import pyro.infer as infer\n",
    "import pyro.optim as optim\n",
    "from pyro.params import param_with_module_name\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "pyro.clear_param_store()\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import numpy\n",
    "import math\n",
    "import cProfile\n",
    "import time\n",
    "torch.manual_seed(101)\n",
    "pyro.set_rng_seed(1)\n",
    "from pyro.infer.mcmc.hmc import HMC\n",
    "from pyro.infer.mcmc import MCMC\n",
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.stack([dist.Normal(4.8, 0.1).sample((150,)),dist.Normal(3.2, 0.3).sample((150,)),dist.Normal(1.5, 0.4).sample((150,)),\n",
    "dist.Exponential(0.5).sample((150,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(y):\n",
    "        X_loc = torch.zeros(y.shape[1], 2)\n",
    "        kernel = gp.kernels.RBF(input_dim=2, lengthscale=torch.ones(2))\n",
    "        Xu = torch.zeros(20, 2)  # initial inducing inputs of sparse model\n",
    "        gpmodel = gp.models.SparseGPRegression(X_loc, y, kernel, Xu, noise=torch.tensor(1.))\n",
    "        X_loc = gpmodel.X\n",
    "        C = X_loc.shape[1]\n",
    "        Id = torch.eye(C, out=X_loc.new_empty(C, C))\n",
    "        zero_loc = X_loc.new_zeros(X_loc.shape)\n",
    "        X = pyro.sample('Var_dist', dist.MultivariateNormal(zero_loc, scale_tril=Id)\n",
    "                                    .independent(zero_loc.dim()-1))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_kernel = HMC(model, step_size=0.0855, num_steps=4,adapt_step_size=True)\n",
    "mcmc_run = MCMC(hmc_kernel, num_samples=500, warmup_steps=100).run(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = pyro.infer.abstract_infer.EmpiricalMarginal(mcmc_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/aneeqr/.local/lib/python3.6/site-packages/torch/distributions/distribution.py:112: UserWarning:sample_n will be deprecated. Use .sample((n,)) instead\n"
     ]
    }
   ],
   "source": [
    "Samples = posterior.sample_n(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_log_pdf = []\n",
    "for tr in mcmc_run.exec_traces:\n",
    "    trace_log_pdf.append(tr.log_prob_sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = numpy.array(y)\n",
    "Y = Y.T # Reshaping vector 150 x 4 for Bayesian GPLVM\n",
    "kernel = GPy.kern.RBF(2, ARD=True,variance=1.0,lengthscale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Scaled Conjugate Gradients Code:\n",
      "  runtime   i      f              |g|        \n",
      "    00s08  0006   1.213564e+03   1.575275e+04 \n",
      "    00s17  0013   1.091472e+03   8.548511e+03 \n",
      "    00s19  0015   8.832638e+02   5.727514e+03 \n",
      "    02s20  0225   4.941202e+02   2.013467e+03 \n",
      "    06s22  0655   4.193528e+02   3.903572e+01 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/aneeqr/.local/lib/python3.6/site-packages/GPy/kern/src/stationary.py:165: RuntimeWarning:overflow encountered in true_divide\n",
      " /home/aneeqr/.local/lib/python3.6/site-packages/GPy/kern/src/stationary.py:137: RuntimeWarning:invalid value encountered in add\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    08s23  0883   4.167489e+02   2.678618e+02 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/aneeqr/.local/lib/python3.6/site-packages/GPy/kern/src/psi_comp/rbf_psi_comp.py:131: RuntimeWarning:invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    14s27  1515   4.140950e+02   1.008188e+01 \n",
      "    32s33  3281   4.138749e+02   9.678437e+00 \n",
      "Runtime:     32s33\n",
      "Optimization status: maxiter exceeded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = GPy.models.BayesianGPLVM(Y, 2,  init=\"PCA\",kernel=kernel,num_inducing=20)\n",
    "m.data_labels = Y.argmax(axis=1)\n",
    "loss = m.optimize('scg', messages=1,max_iters=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Joint_Prob(Y,Xs,N,D,Q,kernel,noise_var=0.05):\n",
    "    K= kernel.K(np.array(Xs),np.array(Xs)) # Covariance matrix for K(Xs,Xs), KNN\n",
    "    Noise=np.eye(K.shape[0])*np.sqrt(noise_var)  # B^-1 Noise vector\n",
    "    Noise_term = np.matmul(np.linalg.inv(Noise),np.eye(K.shape[0]))  # B^-1 I\n",
    "    K_new = np.add(K,Noise_term) #  KNN + B^(-1)I_{N}\n",
    "    K_inv = np.linalg.inv(K_new) # PNN inverse of KNN\n",
    "    Prob = -N*D*0.5*np.log(2*np.pi) - D/2*np.log(np.linalg.det(K_new))-1/2*np.trace(np.matmul(np.matmul(Y.T,K_inv),Y)) - (N*Q/2)*np.log(2*np.pi)-0.5*np.trace(np.matmul(Xs,Xs.T))\n",
    "    return Prob \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N= y.shape[1]\n",
    "Q = 2\n",
    "Join = []\n",
    "for sample in Samples:\n",
    "    Join.append(Joint_Prob(Y,sample.detach().numpy(),N=N,D=4,Q=2,kernel=kernel,noise_var=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "VS= list(np.array(trace_log_pdf).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rs = [] # Rations with exp\n",
    "Rs2 = [] # Rations without exp just log ratios\n",
    "S = len(Join)\n",
    "for i in range(S):\n",
    "    Rs.append(np.exp(Join[i]-VS[i]))\n",
    "    Rs2.append(Join[i]-VS[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:(-885.1626124159513, -885.1626124159513, 6.88609165416292)\n"
     ]
    }
   ],
   "source": [
    "print('K:'+ str(psisloo(np.array(Rs2)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyro.infer.mcmc.nuts as NUTS\n",
    "from pyro.infer.mcmc import MCMC, NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.stack([dist.Normal(4.8, 0.1).sample((150,)),dist.Normal(3.2, 0.3).sample((150,)),dist.Normal(1.5, 0.4).sample((150,)),\n",
    "dist.Exponential(0.5).sample((150,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_kernel2 = NUTS(model, step_size=0.0855, adapt_step_size=True)\n",
    "mcmc_run2 = MCMC(hmc_kernel, num_samples=500, warmup_steps=100).run(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = pyro.infer.abstract_infer.EmpiricalMarginal(mcmc_run2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/aneeqr/.local/lib/python3.6/site-packages/torch/distributions/distribution.py:112: UserWarning:sample_n will be deprecated. Use .sample((n,)) instead\n"
     ]
    }
   ],
   "source": [
    "Samples = posterior.sample_n(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_log_pdf = []\n",
    "for tr in mcmc_run2.exec_traces:\n",
    "    trace_log_pdf.append(tr.log_prob_sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "VS= list(np.array(trace_log_pdf).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rs = [] # Rations with exp\n",
    "Rs2 = [] # Rations without exp just log ratios\n",
    "S = len(Join)\n",
    "for i in range(S):\n",
    "    Rs.append(np.exp(Join[i]-VS[i]))\n",
    "    Rs2.append(Join[i]-VS[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:(-887.9659141686412, -887.9659141686412, 7.220496658394163)\n"
     ]
    }
   ],
   "source": [
    "print('K:'+ str(psisloo(np.array(Rs2)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psisloo(log_lik, **kwargs):\n",
    "    r\"\"\"PSIS leave-one-out log predictive densities.\n",
    "\n",
    "    Computes the log predictive densities given posterior samples of the log\n",
    "    likelihood terms :math:`p(y_i|\\theta^s)` in input parameter `log_lik`.\n",
    "    Returns a sum of the leave-one-out log predictive densities `loo`,\n",
    "    individual leave-one-out log predictive density terms `loos` and an estimate\n",
    "    of Pareto tail indeces `ks`. The estimates are unreliable if tail index\n",
    "    ``k > 0.7`` (see more in the references listed in the module docstring).\n",
    "\n",
    "    Additional keyword arguments are passed to the :meth:`psislw()` function\n",
    "    (see the corresponding documentation).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_lik : ndarray\n",
    "        Array of size n x m containing n posterior samples of the log likelihood\n",
    "        terms :math:`p(y_i|\\theta^s)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loo : scalar\n",
    "        sum of the leave-one-out log predictive densities\n",
    "\n",
    "    loos : ndarray\n",
    "        individual leave-one-out log predictive density terms\n",
    "\n",
    "    ks : ndarray\n",
    "        estimated Pareto tail indeces\n",
    "\n",
    "    \"\"\"\n",
    "    # ensure overwrite flag in passed arguments\n",
    "    kwargs['overwrite_lw'] = True\n",
    "    # log raw weights from log_lik\n",
    "    lw = -log_lik\n",
    "    # compute Pareto smoothed log weights given raw log weights\n",
    "    lw, ks = psislw(lw, **kwargs)\n",
    "    # compute\n",
    "    lw += log_lik\n",
    "    loos = sumlogs(lw, axis=0)\n",
    "    loo = loos.sum()\n",
    "    #print('Ks'+str(ks))\n",
    "    return loo, loos, ks\n",
    "\n",
    "def psislw(lw, Reff=1.0, overwrite_lw=False):\n",
    "    \"\"\"Pareto smoothed importance sampling (PSIS).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lw : ndarray\n",
    "        Array of size n x m containing m sets of n log weights. It is also\n",
    "        possible to provide one dimensional array of length n.\n",
    "\n",
    "    Reff : scalar, optional\n",
    "        relative MCMC efficiency ``N_eff / N``\n",
    "\n",
    "    overwrite_lw : bool, optional\n",
    "        If True, the input array `lw` is smoothed in-place, assuming the array\n",
    "        is F-contiguous. By default, a new array is allocated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lw_out : ndarray\n",
    "        smoothed log weights\n",
    "    kss : ndarray\n",
    "        Pareto tail indices\n",
    "\n",
    "    \"\"\"\n",
    "    if lw.ndim == 2:\n",
    "        n, m = lw.shape\n",
    "    elif lw.ndim == 1:\n",
    "        n = len(lw)\n",
    "        m = 1\n",
    "    else:\n",
    "        raise ValueError(\"Argument `lw` must be 1 or 2 dimensional.\")\n",
    "    if n <= 1:\n",
    "        raise ValueError(\"More than one log-weight needed.\")\n",
    "\n",
    "    if overwrite_lw and lw.flags.f_contiguous:\n",
    "        # in-place operation\n",
    "        lw_out = lw\n",
    "    else:\n",
    "        # allocate new array for output\n",
    "        lw_out = np.copy(lw, order='F')\n",
    "\n",
    "    # allocate output array for kss\n",
    "    kss = np.empty(m)\n",
    "    #print(kss)\n",
    "\n",
    "    # precalculate constants\n",
    "    cutoff_ind = - int(np.ceil(min(0.2 * n, 3 * np.sqrt(n / Reff)))) - 1\n",
    "    cutoffmin = np.log(np.finfo(float).tiny)\n",
    "    logn = np.log(n)\n",
    "    k_min = 1/3\n",
    "\n",
    "    # loop over sets of log weights\n",
    "    for i, x in enumerate(lw_out.T if lw_out.ndim == 2 else lw_out[None, :]):\n",
    "        # improve numerical accuracy\n",
    "        x -= np.max(x)\n",
    "        # sort the array\n",
    "        x_sort_ind = np.argsort(x)\n",
    "        # divide log weights into body and right tail\n",
    "        xcutoff = max(\n",
    "            x[x_sort_ind[cutoff_ind]],\n",
    "            cutoffmin\n",
    "        )\n",
    "        expxcutoff = np.exp(xcutoff)\n",
    "        tailinds, = np.where(x > xcutoff)\n",
    "        x2 = x[tailinds]\n",
    "        n2 = len(x2)\n",
    "        if n2 <= 4:\n",
    "            # not enough tail samples for gpdfitnew\n",
    "            k = np.inf\n",
    "        else:\n",
    "            # order of tail samples\n",
    "            x2si = np.argsort(x2)\n",
    "            # fit generalized Pareto distribution to the right tail samples\n",
    "            np.exp(x2, out=x2)\n",
    "            x2 -= expxcutoff\n",
    "            k, sigma = gpdfitnew(x2, sort=x2si)\n",
    "        if k >= k_min and not np.isinf(k):\n",
    "            # no smoothing if short tail or GPD fit failed\n",
    "            # compute ordered statistic for the fit\n",
    "            sti = np.arange(0.5, n2)\n",
    "            sti /= n2\n",
    "            qq = gpinv(sti, k, sigma)\n",
    "            qq += expxcutoff\n",
    "            np.log(qq, out=qq)\n",
    "            # place the smoothed tail into the output array\n",
    "            x[tailinds[x2si]] = qq\n",
    "            # truncate smoothed values to the largest raw weight 0\n",
    "            x[x > 0] = 0\n",
    "        # renormalize weights\n",
    "        x -= sumlogs(x)\n",
    "        # store tail index k\n",
    "        kss[i] = k\n",
    "        #print(k)\n",
    "\n",
    "    # If the provided input array is one dimensional, return kss as scalar.\n",
    "    if lw_out.ndim == 1:\n",
    "        kss = kss[0]\n",
    "\n",
    "    return lw_out, kss\n",
    "\n",
    "def sumlogs(x, axis=None, out=None):\n",
    "    \"\"\"Sum of vector where numbers are represented by their logarithms.\n",
    "\n",
    "    Calculates ``np.log(np.sum(np.exp(x), axis=axis))`` in such a fashion that\n",
    "    it works even when elements have large magnitude.\n",
    "\n",
    "    \"\"\"\n",
    "    maxx = x.max(axis=axis, keepdims=True)\n",
    "    xnorm = x - maxx\n",
    "    np.exp(xnorm, out=xnorm)\n",
    "    out = np.sum(xnorm, axis=axis, out=out)\n",
    "    if isinstance(out, np.ndarray):\n",
    "        np.log(out, out=out)\n",
    "    else:\n",
    "        out = np.log(out)\n",
    "    out += np.squeeze(maxx)\n",
    "    return out\n",
    "def gpdfitnew(x, sort=True, sort_in_place=False, return_quadrature=False):\n",
    "    \"\"\"Estimate the paramaters for the Generalized Pareto Distribution (GPD)\n",
    "\n",
    "    Returns empirical Bayes estimate for the parameters of the two-parameter\n",
    "    generalized Parato distribution given the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        One dimensional data array\n",
    "\n",
    "    sort : bool or ndarray, optional\n",
    "        If known in advance, one can provide an array of indices that would\n",
    "        sort the input array `x`. If the input array is already sorted, provide\n",
    "        False. If True (default behaviour), the array is sorted internally.\n",
    "\n",
    "    sort_in_place : bool, optional\n",
    "        If `sort` is True and `sort_in_place` is True, the array is sorted\n",
    "        in-place (False by default).\n",
    "\n",
    "    return_quadrature : bool, optional\n",
    "        If True, quadrature points and weight `ks` and `w` of the marginal posterior distribution of k are also calculated and returned. False by\n",
    "        default.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    k, sigma : float\n",
    "        estimated parameter values\n",
    "\n",
    "    ks, w : ndarray\n",
    "        Quadrature points and weights of the marginal posterior distribution\n",
    "        of `k`. Returned only if `return_quadrature` is True.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function returns a negative of Zhang and Stephens's k, because it is\n",
    "    more common parameterisation.\n",
    "\n",
    "    \"\"\"\n",
    "    if x.ndim != 1 or len(x) <= 1:\n",
    "        raise ValueError(\"Invalid input array.\")\n",
    "\n",
    "    # check if x should be sorted\n",
    "    if sort is True:\n",
    "        if sort_in_place:\n",
    "            x.sort()\n",
    "            xsorted = True\n",
    "        else:\n",
    "            sort = np.argsort(x)\n",
    "            xsorted = False\n",
    "    elif sort is False:\n",
    "        xsorted = True\n",
    "    else:\n",
    "        xsorted = False\n",
    "\n",
    "    n = len(x)\n",
    "    PRIOR = 3\n",
    "    m = 30 + int(np.sqrt(n))\n",
    "\n",
    "    bs = np.arange(1, m + 1, dtype=float)\n",
    "    bs -= 0.5\n",
    "    np.divide(m, bs, out=bs)\n",
    "    np.sqrt(bs, out=bs)\n",
    "    np.subtract(1, bs, out=bs)\n",
    "    if xsorted:\n",
    "        bs /= PRIOR * x[int(n/4 + 0.5) - 1]\n",
    "        bs += 1 / x[-1]\n",
    "    else:\n",
    "        bs /= PRIOR * x[sort[int(n/4 + 0.5) - 1]]\n",
    "        bs += 1 / x[sort[-1]]\n",
    "\n",
    "    ks = np.negative(bs)\n",
    "    temp = ks[:,None] * x\n",
    "    np.log1p(temp, out=temp)\n",
    "    np.mean(temp, axis=1, out=ks)\n",
    "\n",
    "    L = bs / ks\n",
    "    np.negative(L, out=L)\n",
    "    np.log(L, out=L)\n",
    "    L -= ks\n",
    "    L -= 1\n",
    "    L *= n\n",
    "\n",
    "    temp = L - L[:,None]\n",
    "    np.exp(temp, out=temp)\n",
    "    w = np.sum(temp, axis=1)\n",
    "    np.divide(1, w, out=w)\n",
    "\n",
    "    # remove negligible weights\n",
    "    dii = w >= 10 * np.finfo(float).eps\n",
    "    if not np.all(dii):\n",
    "        w = w[dii]\n",
    "        bs = bs[dii]\n",
    "    # normalise w\n",
    "    w /= w.sum()\n",
    "\n",
    "    # posterior mean for b\n",
    "    b = np.sum(bs * w)\n",
    "    # Estimate for k, note that we return a negative of Zhang and\n",
    "    # Stephens's k, because it is more common parameterisation.\n",
    "    temp = (-b) * x\n",
    "    np.log1p(temp, out=temp)\n",
    "    k = np.mean(temp)\n",
    "    if return_quadrature:\n",
    "        np.negative(x, out=temp)\n",
    "        temp = bs[:, None] * temp\n",
    "        np.log1p(temp, out=temp)\n",
    "        ks = np.mean(temp, axis=1)\n",
    "    # estimate for sigma\n",
    "    sigma = -k / b * n / (n - 0)\n",
    "    # weakly informative prior for k\n",
    "    a = 10\n",
    "    k = k * n / (n+a) + a * 0.5 / (n+a)\n",
    "    if return_quadrature:\n",
    "        ks *= n / (n+a)\n",
    "        ks += a * 0.5 / (n+a)\n",
    "\n",
    "    if return_quadrature:\n",
    "        #print('K:'+str(k))\n",
    "        return k, sigma, ks, w\n",
    "    else:\n",
    "        #print('K:'+str(k))\n",
    "        return k, sigma\n",
    "\n",
    "def gpinv(p, k, sigma):\n",
    "    \"\"\"Inverse Generalised Pareto distribution function.\"\"\"\n",
    "    x = np.empty(p.shape)\n",
    "    x.fill(np.nan)\n",
    "    if sigma <= 0:\n",
    "        return x\n",
    "    ok = (p > 0) & (p < 1)\n",
    "    if np.all(ok):\n",
    "        if np.abs(k) < np.finfo(float).eps:\n",
    "            np.negative(p, out=x)\n",
    "            np.log1p(x, out=x)\n",
    "            np.negative(x, out=x)\n",
    "        else:\n",
    "            np.negative(p, out=x)\n",
    "            np.log1p(x, out=x)\n",
    "            x *= -k\n",
    "            np.expm1(x, out=x)\n",
    "            x /= k\n",
    "        x *= sigma\n",
    "    else:\n",
    "        if np.abs(k) < np.finfo(float).eps:\n",
    "            # x[ok] = - np.log1p(-p[ok])\n",
    "            temp = p[ok]\n",
    "            np.negative(temp, out=temp)\n",
    "            np.log1p(temp, out=temp)\n",
    "            np.negative(temp, out=temp)\n",
    "            x[ok] = temp\n",
    "        else:\n",
    "            # x[ok] = np.expm1(-k * np.log1p(-p[ok])) / k\n",
    "            temp = p[ok]\n",
    "            np.negative(temp, out=temp)\n",
    "            np.log1p(temp, out=temp)\n",
    "            temp *= -k\n",
    "            np.expm1(temp, out=temp)\n",
    "            temp /= k\n",
    "            x[ok] = temp\n",
    "        x *= sigma\n",
    "        x[p == 0] = 0\n",
    "        if k >= 0:\n",
    "            x[p == 1] = np.inf\n",
    "        else:\n",
    "            x[p == 1] = -sigma / k\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
