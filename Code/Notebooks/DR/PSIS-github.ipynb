{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "from torch.nn import Parameter\n",
    "import pyro\n",
    "from pyro.contrib.gp.util import Parameterized\n",
    "import pyro.distributions as dist\n",
    "import pyro.infer as infer\n",
    "import pyro.optim as optim\n",
    "from pyro.params import param_with_module_name\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "pyro.clear_param_store()\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import numpy\n",
    "import math\n",
    "import cProfile\n",
    "import time\n",
    "torch.manual_seed(101)\n",
    "pyro.set_rng_seed(1)\n",
    "import GPy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.stack([dist.Normal(4.8, 0.1).sample((150,)),dist.Normal(3.2, 0.3).sample((150,)),dist.Normal(1.5, 0.4).sample((150,)),\n",
    " dist.Exponential(0.5).sample((150,))])\n",
    "X_loc = torch.zeros(150, 2)\n",
    "kernel = gp.kernels.RBF(input_dim=2,lengthscale=torch.ones(1))\n",
    "Xu = torch.zeros(20, 2)\n",
    "gpmodel = gp.models.SparseGPRegression(X_loc, y, kernel, Xu, noise=torch.tensor(1.))\n",
    "gplvm = gp.models.GPLVM(gpmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = gplvm.optimize(num_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = gplvm.get_param(\"X_loc\")\n",
    "X = X.detach().numpy()\n",
    "X_var = gplvm.get_param(\"X_scale_tril\")\n",
    "X_var = X_var.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from a variational distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Samples=150\n",
    "VS = []\n",
    "for i in range(Samples):\n",
    "    VS.append(pyro.sample('Variational Distibtuion',dist.MultivariateNormal(gplvm.get_param(\"X_loc\"), scale_tril=gplvm.get_param(\"X_scale_tril\")).independent(gplvm.get_param(\"X_loc\").dim()-1),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian GPLVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = numpy.array(y)\n",
    "Y = Y.T\n",
    "kernel = GPy.kern.RBF(2, ARD=True,variance=1.0,lengthscale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Scaled Conjugate Gradients Code:\n",
      "  runtime   i      f              |g|        \n",
      "    00s16  0004   1.213564e+03   4.125049e+04 \n",
      "    02s21  0065   5.542470e+02   4.418814e+03 \n",
      "    03s23  0095   5.448641e+02   2.012194e+02 \n",
      "    10s36  0324   4.654211e+02   3.912429e+01 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/aneeqr/.local/lib/python3.6/site-packages/GPy/kern/src/stationary.py:165: RuntimeWarning:overflow encountered in true_divide\n",
      " /home/aneeqr/.local/lib/python3.6/site-packages/GPy/kern/src/stationary.py:137: RuntimeWarning:invalid value encountered in add\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    15s45  0445   4.618513e+02   2.896723e+01 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/aneeqr/.local/lib/python3.6/site-packages/GPy/kern/src/stationary.py:136: RuntimeWarning:overflow encountered in square\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    31s73  0962   4.408334e+02   2.026809e+01 \n",
      "    47s58  1455   4.046699e+02   1.885049e+01 \n",
      "Runtime:     47s58\n",
      "Optimization status: converged - relative stepsize\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = GPy.models.BayesianGPLVM(Y, 2,  init=\"PCA\",kernel=kernel,num_inducing=20)\n",
    "m.data_labels = Y.argmax(axis=1)\n",
    "loss = m.optimize('scg', messages=1,max_iters=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settting up the Covariance Matrix for Joint PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate Joint PDF as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(\\mathbf{y}|\\mathbf{X}) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\mathbf{K}|^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}\\mathbf{y}^\\top \\left(\\mathbf{K}+\\sigma^2 \\mathbf{I}\\right)^{-1}\\mathbf{y}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Joint_Prob(y,X,kernel,D):\n",
    "    K= kernel.K(np.array(X),np.array(X)) # Covariance matrix for X prior\n",
    "    noise_var = 0.05\n",
    "    Noise=np.eye(K.shape[0])*np.sqrt(noise_var) \n",
    "    Noise_term = np.matmul(np.linalg.inv(Noise),np.eye(K.shape[0]))  # Noise term\n",
    "    K_new = np.add(K,Noise_term) #  Covariance matrix for\n",
    "    p_yx=1\n",
    "    for i in range(D):\n",
    "        p_yx = p_yx*(1/(np.sqrt(2*np.pi)**(int(K_new.shape[0]/2))*np.linalg.det(K_new)))*np.exp(-1/2*np.matmul(np.matmul(y[:,i].reshape(K_new.shape[0],1).T,np.linalg.inv(K_new)),y[:,i].reshape(K_new.shape[0],1)))\n",
    "    return p_yx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate variational distribution as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q(\\mathbf{X}) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\mathbf{K}|^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2}\\mathbf{x}^\\top \\left(\\mathbf{K}+\\sigma^2 \\mathbf{I}\\right)^{-1}\\mathbf{x}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Variational_Dist(X,D,kernel):\n",
    "    qx = 1\n",
    "    K= kernel.K(np.array(X),np.array(X)) # Covariance matrix for X prior\n",
    "    noise_var = 0.05\n",
    "    Noise=np.eye(K.shape[0])*np.sqrt(noise_var) \n",
    "    Noise_term = np.matmul(np.linalg.inv(Noise),np.eye(K.shape[0]))  # Noise term\n",
    "    K_new = np.add(K,Noise_term) #  Covariance matrix for\n",
    "    qx=1\n",
    "    for i in range(D):\n",
    "        qx = qx*(1/(np.sqrt(2*np.pi)**(int(K_new.shape[0]/2))*np.linalg.det(K_new)))*np.exp(-1/2*np.matmul(np.matmul(X[:,i].reshape(K_new.shape[0],1).T,np.linalg.inv(K_new)),X[:,i].reshape(K_new.shape[0],1)))\n",
    "    return qx  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSIS Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psislw(lw, Reff=1.0, overwrite_lw=False):\n",
    "    \"\"\"Pareto smoothed importance sampling (PSIS).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lw : ndarray\n",
    "        Array of size n x m containing m sets of n log weights. It is also\n",
    "        possible to provide one dimensional array of length n.\n",
    "\n",
    "    Reff : scalar, optional\n",
    "        relative MCMC efficiency ``N_eff / N``\n",
    "\n",
    "    overwrite_lw : bool, optional\n",
    "        If True, the input array `lw` is smoothed in-place, assuming the array\n",
    "        is F-contiguous. By default, a new array is allocated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lw_out : ndarray\n",
    "        smoothed log weights\n",
    "    kss : ndarray\n",
    "        Pareto tail indices\n",
    "\n",
    "    \"\"\"\n",
    "    if lw.ndim == 2:\n",
    "        n, m = lw.shape\n",
    "    elif lw.ndim == 1:\n",
    "        n = len(lw)\n",
    "        m = 1\n",
    "    else:\n",
    "        raise ValueError(\"Argument `lw` must be 1 or 2 dimensional.\")\n",
    "    if n <= 1:\n",
    "        raise ValueError(\"More than one log-weight needed.\")\n",
    "\n",
    "    if overwrite_lw and lw.flags.f_contiguous:\n",
    "        # in-place operation\n",
    "        lw_out = lw\n",
    "    else:\n",
    "        # allocate new array for output\n",
    "        lw_out = np.copy(lw, order='F')\n",
    "\n",
    "    # allocate output array for kss\n",
    "    kss = np.empty(m)\n",
    "    #print(kss)\n",
    "\n",
    "    # precalculate constants\n",
    "    cutoff_ind = - int(np.ceil(min(0.2 * n, 3 * np.sqrt(n / Reff)))) - 1\n",
    "    cutoffmin = np.log(np.finfo(float).tiny)\n",
    "    logn = np.log(n)\n",
    "    k_min = 1/3\n",
    "\n",
    "    # loop over sets of log weights\n",
    "    for i, x in enumerate(lw_out.T if lw_out.ndim == 2 else lw_out[None, :]):\n",
    "        # improve numerical accuracy\n",
    "        x -= np.max(x)\n",
    "        # sort the array\n",
    "        x_sort_ind = np.argsort(x)\n",
    "        # divide log weights into body and right tail\n",
    "        xcutoff = max(\n",
    "            x[x_sort_ind[cutoff_ind]],\n",
    "            cutoffmin\n",
    "        )\n",
    "        expxcutoff = np.exp(xcutoff)\n",
    "        tailinds, = np.where(x > xcutoff)\n",
    "        x2 = x[tailinds]\n",
    "        n2 = len(x2)\n",
    "        if n2 <= 4:\n",
    "            # not enough tail samples for gpdfitnew\n",
    "            k = np.inf\n",
    "        else:\n",
    "            # order of tail samples\n",
    "            x2si = np.argsort(x2)\n",
    "            # fit generalized Pareto distribution to the right tail samples\n",
    "            np.exp(x2, out=x2)\n",
    "            x2 -= expxcutoff\n",
    "            k, sigma = gpdfitnew(x2, sort=x2si)\n",
    "        if k >= k_min and not np.isinf(k):\n",
    "            # no smoothing if short tail or GPD fit failed\n",
    "            # compute ordered statistic for the fit\n",
    "            sti = np.arange(0.5, n2)\n",
    "            sti /= n2\n",
    "            qq = gpinv(sti, k, sigma)\n",
    "            qq += expxcutoff\n",
    "            np.log(qq, out=qq)\n",
    "            # place the smoothed tail into the output array\n",
    "            x[tailinds[x2si]] = qq\n",
    "            # truncate smoothed values to the largest raw weight 0\n",
    "            x[x > 0] = 0\n",
    "        # renormalize weights\n",
    "        x -= sumlogs(x)\n",
    "        # store tail index k\n",
    "        kss[i] = k\n",
    "        #print(k)\n",
    "\n",
    "    # If the provided input array is one dimensional, return kss as scalar.\n",
    "    if lw_out.ndim == 1:\n",
    "        kss = kss[0]\n",
    "\n",
    "    return lw_out, kss\n",
    "\n",
    "def sumlogs(x, axis=None, out=None):\n",
    "    \"\"\"Sum of vector where numbers are represented by their logarithms.\n",
    "\n",
    "    Calculates ``np.log(np.sum(np.exp(x), axis=axis))`` in such a fashion that\n",
    "    it works even when elements have large magnitude.\n",
    "\n",
    "    \"\"\"\n",
    "    maxx = x.max(axis=axis, keepdims=True)\n",
    "    xnorm = x - maxx\n",
    "    np.exp(xnorm, out=xnorm)\n",
    "    out = np.sum(xnorm, axis=axis, out=out)\n",
    "    if isinstance(out, np.ndarray):\n",
    "        np.log(out, out=out)\n",
    "    else:\n",
    "        out = np.log(out)\n",
    "    out += np.squeeze(maxx)\n",
    "    return out\n",
    "def gpdfitnew(x, sort=True, sort_in_place=False, return_quadrature=False):\n",
    "    \"\"\"Estimate the paramaters for the Generalized Pareto Distribution (GPD)\n",
    "\n",
    "    Returns empirical Bayes estimate for the parameters of the two-parameter\n",
    "    generalized Parato distribution given the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        One dimensional data array\n",
    "\n",
    "    sort : bool or ndarray, optional\n",
    "        If known in advance, one can provide an array of indices that would\n",
    "        sort the input array `x`. If the input array is already sorted, provide\n",
    "        False. If True (default behaviour), the array is sorted internally.\n",
    "\n",
    "    sort_in_place : bool, optional\n",
    "        If `sort` is True and `sort_in_place` is True, the array is sorted\n",
    "        in-place (False by default).\n",
    "\n",
    "    return_quadrature : bool, optional\n",
    "        If True, quadrature points and weight `ks` and `w` of the marginal posterior distribution of k are also calculated and returned. False by\n",
    "        default.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    k, sigma : float\n",
    "        estimated parameter values\n",
    "\n",
    "    ks, w : ndarray\n",
    "        Quadrature points and weights of the marginal posterior distribution\n",
    "        of `k`. Returned only if `return_quadrature` is True.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function returns a negative of Zhang and Stephens's k, because it is\n",
    "    more common parameterisation.\n",
    "\n",
    "    \"\"\"\n",
    "    if x.ndim != 1 or len(x) <= 1:\n",
    "        raise ValueError(\"Invalid input array.\")\n",
    "\n",
    "    # check if x should be sorted\n",
    "    if sort is True:\n",
    "        if sort_in_place:\n",
    "            x.sort()\n",
    "            xsorted = True\n",
    "        else:\n",
    "            sort = np.argsort(x)\n",
    "            xsorted = False\n",
    "    elif sort is False:\n",
    "        xsorted = True\n",
    "    else:\n",
    "        xsorted = False\n",
    "\n",
    "    n = len(x)\n",
    "    PRIOR = 3\n",
    "    m = 30 + int(np.sqrt(n))\n",
    "\n",
    "    bs = np.arange(1, m + 1, dtype=float)\n",
    "    bs -= 0.5\n",
    "    np.divide(m, bs, out=bs)\n",
    "    np.sqrt(bs, out=bs)\n",
    "    np.subtract(1, bs, out=bs)\n",
    "    if xsorted:\n",
    "        bs /= PRIOR * x[int(n/4 + 0.5) - 1]\n",
    "        bs += 1 / x[-1]\n",
    "    else:\n",
    "        bs /= PRIOR * x[sort[int(n/4 + 0.5) - 1]]\n",
    "        bs += 1 / x[sort[-1]]\n",
    "\n",
    "    ks = np.negative(bs)\n",
    "    temp = ks[:,None] * x\n",
    "    np.log1p(temp, out=temp)\n",
    "    np.mean(temp, axis=1, out=ks)\n",
    "\n",
    "    L = bs / ks\n",
    "    np.negative(L, out=L)\n",
    "    np.log(L, out=L)\n",
    "    L -= ks\n",
    "    L -= 1\n",
    "    L *= n\n",
    "\n",
    "    temp = L - L[:,None]\n",
    "    np.exp(temp, out=temp)\n",
    "    w = np.sum(temp, axis=1)\n",
    "    np.divide(1, w, out=w)\n",
    "\n",
    "    # remove negligible weights\n",
    "    dii = w >= 10 * np.finfo(float).eps\n",
    "    if not np.all(dii):\n",
    "        w = w[dii]\n",
    "        bs = bs[dii]\n",
    "    # normalise w\n",
    "    w /= w.sum()\n",
    "\n",
    "    # posterior mean for b\n",
    "    b = np.sum(bs * w)\n",
    "    # Estimate for k, note that we return a negative of Zhang and\n",
    "    # Stephens's k, because it is more common parameterisation.\n",
    "    temp = (-b) * x\n",
    "    np.log1p(temp, out=temp)\n",
    "    k = np.mean(temp)\n",
    "    if return_quadrature:\n",
    "        np.negative(x, out=temp)\n",
    "        temp = bs[:, None] * temp\n",
    "        np.log1p(temp, out=temp)\n",
    "        ks = np.mean(temp, axis=1)\n",
    "    # estimate for sigma\n",
    "    sigma = -k / b * n / (n - 0)\n",
    "    # weakly informative prior for k\n",
    "    a = 10\n",
    "    k = k * n / (n+a) + a * 0.5 / (n+a)\n",
    "    if return_quadrature:\n",
    "        ks *= n / (n+a)\n",
    "        ks += a * 0.5 / (n+a)\n",
    "\n",
    "    if return_quadrature:\n",
    "        #print('K:'+str(k))\n",
    "        return k, sigma, ks, w\n",
    "    else:\n",
    "        #print('K:'+str(k))\n",
    "        return k, sigma\n",
    "\n",
    "def gpinv(p, k, sigma):\n",
    "    \"\"\"Inverse Generalised Pareto distribution function.\"\"\"\n",
    "    x = np.empty(p.shape)\n",
    "    x.fill(np.nan)\n",
    "    if sigma <= 0:\n",
    "        return x\n",
    "    ok = (p > 0) & (p < 1)\n",
    "    if np.all(ok):\n",
    "        if np.abs(k) < np.finfo(float).eps:\n",
    "            np.negative(p, out=x)\n",
    "            np.log1p(x, out=x)\n",
    "            np.negative(x, out=x)\n",
    "        else:\n",
    "            np.negative(p, out=x)\n",
    "            np.log1p(x, out=x)\n",
    "            x *= -k\n",
    "            np.expm1(x, out=x)\n",
    "            x /= k\n",
    "        x *= sigma\n",
    "    else:\n",
    "        if np.abs(k) < np.finfo(float).eps:\n",
    "            # x[ok] = - np.log1p(-p[ok])\n",
    "            temp = p[ok]\n",
    "            np.negative(temp, out=temp)\n",
    "            np.log1p(temp, out=temp)\n",
    "            np.negative(temp, out=temp)\n",
    "            x[ok] = temp\n",
    "        else:\n",
    "            # x[ok] = np.expm1(-k * np.log1p(-p[ok])) / k\n",
    "            temp = p[ok]\n",
    "            np.negative(temp, out=temp)\n",
    "            np.log1p(temp, out=temp)\n",
    "            temp *= -k\n",
    "            np.expm1(temp, out=temp)\n",
    "            temp /= k\n",
    "            x[ok] = temp\n",
    "        x *= sigma\n",
    "        x[p == 0] = 0\n",
    "        if k >= 0:\n",
    "            x[p == 1] = np.inf\n",
    "        else:\n",
    "            x[p == 1] = -sigma / k\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Importance Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ratio=[]\n",
    "D=2\n",
    "N=4\n",
    "for X in VS:\n",
    "    P = np.log(np.exp(Joint_Prob(Y,X.detach().numpy(),kernel,N)))\n",
    "    Q = np.exp(Variational_Dist(X.detach().numpy(),D,kernel))\n",
    "    Ratio.append(np.divide(P,Q).flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ratio = np.array(Ratio)\n",
    "Ratio = Ratio.reshape(-1)\n",
    "Ratio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/aneeqr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:186: RuntimeWarning:divide by zero encountered in true_divide\n",
      " /home/aneeqr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:187: RuntimeWarning:divide by zero encountered in double_scalars\n",
      " /home/aneeqr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:187: RuntimeWarning:invalid value encountered in add\n",
      " /home/aneeqr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:207: RuntimeWarning:invalid value encountered in greater_equal\n",
      " /home/aneeqr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:227: RuntimeWarning:invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.03125, nan)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpdfitnew(Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
