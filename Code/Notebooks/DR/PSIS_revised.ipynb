{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "from torch.nn import Parameter\n",
    "import pyro\n",
    "from pyro.contrib.gp.util import Parameterized\n",
    "import pyro.distributions as dist\n",
    "import pyro.infer as infer\n",
    "import pyro.optim as optim\n",
    "from pyro.params import param_with_module_name\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "pyro.clear_param_store()\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import numpy\n",
    "import math\n",
    "import cProfile\n",
    "import time\n",
    "torch.manual_seed(101)\n",
    "pyro.set_rng_seed(1)\n",
    "import GPy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset\n",
    "y = torch.stack([dist.Normal(4.8, 0.1).sample((150,)),dist.Normal(3.2, 0.3).sample((150,)),dist.Normal(1.5, 0.4).sample((150,)),\n",
    " dist.Exponential(0.5).sample((150,))])\n",
    "# Fixed Inducing Inputs for Sparse Model\n",
    "X_loc = torch.zeros(150, 2)\n",
    "# Kenel\n",
    "kernel = gp.kernels.RBF(input_dim=2,lengthscale=torch.ones(1))\n",
    "Xu = torch.zeros(20, 2)\n",
    "# Wrap Sparse GP Model below into GPLVM\n",
    "gpmodel = gp.models.SparseGPRegression(X_loc, y, kernel, Xu, noise=torch.tensor(1.))\n",
    "gplvm = gp.models.GPLVM(gpmodel)\n",
    "# Optimize Bayesian GPLVM\n",
    "losses = gplvm.optimize(num_steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Variational Parameters X_loc and X_scale_tril from VI approximation\n",
    "X = gplvm.get_param(\"X_loc\")\n",
    "X = X.detach().numpy()\n",
    "X_var = gplvm.get_param(\"X_scale_tril\")\n",
    "X_var = X_var.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Samples from the Variational Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=500\n",
    "VS = [] # Vector Containing Matrix Samples (Xs)\n",
    "for i in range(Samples):\n",
    "    VS.append(pyro.sample('Variational Distibtuion',dist.MultivariateNormal(gplvm.get_param(\"X_loc\"), scale_tril=gplvm.get_param(\"X_scale_tril\")).independent(gplvm.get_param(\"X_loc\").dim()-1),))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian GPLVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = numpy.array(y)\n",
    "Y = Y.T # Reshaping vector 150 x 4 for Bayesian GPLVM\n",
    "kernel = GPy.kern.RBF(2, ARD=True,variance=1.0,lengthscale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Scaled Conjugate Gradients Code:\n",
      "  runtime   i      f              |g|        \n",
      "    00s07  0006   1.213564e+03   1.575275e+04 \n",
      "    00s16  0013   1.091467e+03   8.581268e+03 \n",
      "    00s19  0016   8.793471e+02   5.741800e+03 \n",
      "    02s20  0261   4.674008e+02   5.849822e+01 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/aneeqr/.local/lib/python3.6/site-packages/GPy/kern/src/stationary.py:165: RuntimeWarning:overflow encountered in true_divide\n",
      " /home/aneeqr/.local/lib/python3.6/site-packages/GPy/kern/src/stationary.py:137: RuntimeWarning:invalid value encountered in add\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "    03s21  0382   4.650911e+02   3.592636e+01 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/aneeqr/.local/lib/python3.6/site-packages/GPy/kern/src/stationary.py:136: RuntimeWarning:overflow encountered in square\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    11s23  1330   4.061977e+02   1.327581e+02 \n",
      "    12s34  1455   4.046699e+02   1.885049e+01 \n",
      "Runtime:     12s34\n",
      "Optimization status: converged - relative stepsize\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = GPy.models.BayesianGPLVM(Y, 2,  init=\"PCA\",kernel=kernel,num_inducing=20)\n",
    "m.data_labels = Y.argmax(axis=1)\n",
    "loss = m.optimize('scg', messages=1,max_iters=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Joing Prob Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(\\mathbf{Y}|\\mathbf{X}) = \\prod_{d=1}^D \\mathcal{N} (\\mathbf{y}_d| \\mathbf{0}, \\mathbf{K}_{NN}+\\beta^{-1}\\mathbf{I}_N),\n",
    "$$\n",
    "$$\n",
    "\\mathcal{N} (\\mathbf{y}_d| \\mathbf{0}, \\mathbf{K}_{NN}+\\beta^{-1}\\mathbf{I}_N) = \\frac{1}{(2\\pi)^{N/2}}\\frac{1}{|\\mathbf{K}_{NN}+\\beta^{-1}\\mathbf{I}_N|^{1/2}}\\exp\\bigg[-\\frac{1}{2}\\mathbf{y}_d^{\\top}\\Big(\\mathbf{K}_{NN}+\\beta^{-1}\\mathbf{I}_N\\Big)^{-1}\\mathbf{y}_d\\bigg]\n",
    "$$\n",
    "$$\n",
    "\\log p(\\mathbf{Y},\\mathbf{X}=\\mathbf{X}_s) =-\\frac{ND}{2}\\log 2\\pi-\\frac{D}{2}\\log\\Big(|\\mathbf{K}_{NN}+\\beta^{-1}\\mathbf{I}_N|\\Big)-\\frac{1}{2}\\operatorname{trace}\\Big(\\mathbf{Y}^{\\top}\\mathbf{P}_{NN}\\mathbf{Y}\\Big)-\\frac{NQ}{2}\\log 2\\pi-\\frac{1}{2}\\operatorname{trace}(\\mathbf{X}_s\\mathbf{X}_s^{\\top}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Joint_Prob(Y,Xs,N,D,Q,kernel,noise_var=0.05):\n",
    "    K= kernel.K(np.array(Xs),np.array(Xs)) # Covariance matrix for K(Xs,Xs), KNN\n",
    "    Noise=np.eye(K.shape[0])*np.sqrt(noise_var)  # B^-1 Noise vector\n",
    "    Noise_term = np.matmul(np.linalg.inv(Noise),np.eye(K.shape[0]))  # B^-1 I\n",
    "    K_new = np.add(K,Noise_term) #  KNN + B^(-1)I_{N}\n",
    "    K_inv = np.linalg.inv(K_new) # PNN inverse of KNN\n",
    "    Prob=1\n",
    "    Prob = -N*D*0.5*np.log(2*np.pi) - D/2*np.log(np.linalg.det(K_new))-1/2*np.trace(np.matmul(np.matmul(Y.T,K_inv),Y)) - (N*Q/2)*np.log(2*np.pi)-0.5*np.trace(np.matmul(Xs,Xs.T))\n",
    "    return Prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "N= y.shape[0]\n",
    "Q = 2\n",
    "Join = []\n",
    "for sample in VS:\n",
    "    Join.append(Joint_Prob(Y,sample.detach().numpy(),N=150,D=4,Q=2,kernel=kernel,noise_var=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating q(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q(\\mathbf{x})= \\frac{1}{\\sqrt { (2\\pi)^n|\\boldsymbol \\Sigma| } }  \\exp\\left(-{1 \\over 2} (\\mathbf{x})^{\\rm T} \\boldsymbol\\Sigma^{-1} ({\\mathbf x})\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol\\Sigma_{S} = (\\mathbf{K}_{S}+\\beta^{-1}\\mathbf{I}_S\\Big)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\log q(\\mathbf{X} = \\mathbf{X_{S}}) = -\\frac{ND}{2}\\log 2\\pi-\\frac{D}{2}\\log\\Big(|\\boldsymbol \\Sigma_{s} |\\Big)-\\frac{1}{2}\\operatorname{trace}\\Big(\\mathbf{X_{S}}^{\\top}\\boldsymbol\\Sigma^{-1}_{S}\\mathbf{X_{S}}\\Big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Var_Dist(gplvm,Sample,D,noise_var=0.05):\n",
    "    # Q = K(Xs,Xu)k(Xu,Xu)^-1\n",
    "    Q = np.matmul(gplvm.base_model.kernel(Sample,gplvm.base_model.Xu).detach().numpy(),np.linalg.inv(gplvm.base_model.kernel(gplvm.base_model.Xu,gplvm.base_model.Xu).detach().numpy()))\n",
    "     # Q = K(Xs,Xu)k(Xu,Xu)^-1 k(Xs,Xu)\n",
    "    Q = np.matmul(Q,gplvm.base_model.kernel(Sample,gplvm.base_model.Xu).detach().numpy().T)\n",
    "    # B^-1 I\n",
    "    Noise=np.eye(Q.shape[0])*np.sqrt(noise_var) \n",
    "    Noise_term = np.matmul(np.linalg.inv(Noise),np.eye(Q.shape[0]))  # Noise term\n",
    "    # Knn/ SigmaS\n",
    "    K_new = np.add(Q,Noise_term) #  Covariance matrix for Y\n",
    "    # Sigma^-1\n",
    "    K_inv = np.linalg.inv(K_new)\n",
    "    temp = np.matmul(np.matmul(Sample.detach().numpy().T,K_inv),Sample.detach().numpy())      \n",
    "    qx = -0.5*(Q.shape[0]*(D))*(np.log(np.pi*2)) - D/2*np.log(np.linalg.det(K_new)) - 0.5*np.trace(temp)\n",
    "    return qx  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "qx = []\n",
    "for sample in VS:\n",
    "    qx.append(Var_Dist(gplvm,sample,D=2,noise_var=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSIS\n",
    "\n",
    "To compute the PSIS diagnostic, we compute \n",
    "$$\n",
    "r_s = \\exp\\bigg(\\log\\bigg(\\frac{p(\\mathbf{Y}|\\mathbf{X}=\\mathbf{X}_s)p(\\mathbf{X}=\\mathbf{X}_s)}{q(\\mathbf{X}=\\mathbf{X}_s)}\\bigg)\\bigg),\n",
    "$$\n",
    "which leads to\n",
    "$$\n",
    "r_s = \\exp\\big(\\log\\big(p(\\mathbf{Y}|\\mathbf{X}=\\mathbf{X}_s)p(\\mathbf{X}=\\mathbf{X}_s)\\big)- \\log\\big(q(\\mathbf{X}=\\mathbf{X}_s)\\big)\\big),\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rs = [] # Rations with exp\n",
    "Rs2 = [] # Rations without exp just log ratios\n",
    "for i in range(S):\n",
    "    Rs.append(np.exp(Join[i]-qx[i]))\n",
    "    Rs2.append(Join[i]-qx[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPD without smoothed log weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "psislw takes log weights and gives smoothed weights and tail indices.\n",
    "Passed these log weights in the gpdfitnew to get shape parameter K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:0.00980392156862745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/aneeqr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:235: RuntimeWarning:invalid value encountered in log1p\n",
      " /home/aneeqr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:251: RuntimeWarning:invalid value encountered in greater_equal\n",
      " /home/aneeqr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:271: RuntimeWarning:invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "print('K:'+ str(gpdfitnew(psislw(np.array(Rs2),Reff=1,overwrite_lw=False)[0])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/aneeqr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:235: RuntimeWarning:invalid value encountered in log1p\n",
      " /home/aneeqr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:240: RuntimeWarning:invalid value encountered in log\n",
      " /home/aneeqr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:251: RuntimeWarning:invalid value encountered in greater_equal\n",
      " /home/aneeqr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:271: RuntimeWarning:invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00980392156862745, nan)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpdfitnew(np.array(Rs2)) # Unsmoothed weights #second parameter gives the sigma value which is nan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One out Predictive Densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:3.5672771392297467\n"
     ]
    }
   ],
   "source": [
    "print('K:'+ str(psisloo(np.array(Rs2))[2])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:inf\n"
     ]
    }
   ],
   "source": [
    "print('K:'+ str(psisloo(np.array(Rs))[2])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psisloo(np.array(Join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psisloo(log_lik, **kwargs):\n",
    "    r\"\"\"PSIS leave-one-out log predictive densities.\n",
    "\n",
    "    Computes the log predictive densities given posterior samples of the log\n",
    "    likelihood terms :math:`p(y_i|\\theta^s)` in input parameter `log_lik`.\n",
    "    Returns a sum of the leave-one-out log predictive densities `loo`,\n",
    "    individual leave-one-out log predictive density terms `loos` and an estimate\n",
    "    of Pareto tail indeces `ks`. The estimates are unreliable if tail index\n",
    "    ``k > 0.7`` (see more in the references listed in the module docstring).\n",
    "\n",
    "    Additional keyword arguments are passed to the :meth:`psislw()` function\n",
    "    (see the corresponding documentation).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_lik : ndarray\n",
    "        Array of size n x m containing n posterior samples of the log likelihood\n",
    "        terms :math:`p(y_i|\\theta^s)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loo : scalar\n",
    "        sum of the leave-one-out log predictive densities\n",
    "\n",
    "    loos : ndarray\n",
    "        individual leave-one-out log predictive density terms\n",
    "\n",
    "    ks : ndarray\n",
    "        estimated Pareto tail indeces\n",
    "\n",
    "    \"\"\"\n",
    "    # ensure overwrite flag in passed arguments\n",
    "    kwargs['overwrite_lw'] = True\n",
    "    # log raw weights from log_lik\n",
    "    lw = -log_lik\n",
    "    # compute Pareto smoothed log weights given raw log weights\n",
    "    lw, ks = psislw(lw, **kwargs)\n",
    "    # compute\n",
    "    lw += log_lik\n",
    "    loos = sumlogs(lw, axis=0)\n",
    "    loo = loos.sum()\n",
    "    #print('Ks'+str(ks))\n",
    "    return loo, loos, ks\n",
    "\n",
    "def psislw(lw, Reff=1.0, overwrite_lw=False):\n",
    "    \"\"\"Pareto smoothed importance sampling (PSIS).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lw : ndarray\n",
    "        Array of size n x m containing m sets of n log weights. It is also\n",
    "        possible to provide one dimensional array of length n.\n",
    "\n",
    "    Reff : scalar, optional\n",
    "        relative MCMC efficiency ``N_eff / N``\n",
    "\n",
    "    overwrite_lw : bool, optional\n",
    "        If True, the input array `lw` is smoothed in-place, assuming the array\n",
    "        is F-contiguous. By default, a new array is allocated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lw_out : ndarray\n",
    "        smoothed log weights\n",
    "    kss : ndarray\n",
    "        Pareto tail indices\n",
    "\n",
    "    \"\"\"\n",
    "    if lw.ndim == 2:\n",
    "        n, m = lw.shape\n",
    "    elif lw.ndim == 1:\n",
    "        n = len(lw)\n",
    "        m = 1\n",
    "    else:\n",
    "        raise ValueError(\"Argument `lw` must be 1 or 2 dimensional.\")\n",
    "    if n <= 1:\n",
    "        raise ValueError(\"More than one log-weight needed.\")\n",
    "\n",
    "    if overwrite_lw and lw.flags.f_contiguous:\n",
    "        # in-place operation\n",
    "        lw_out = lw\n",
    "    else:\n",
    "        # allocate new array for output\n",
    "        lw_out = np.copy(lw, order='F')\n",
    "\n",
    "    # allocate output array for kss\n",
    "    kss = np.empty(m)\n",
    "    #print(kss)\n",
    "\n",
    "    # precalculate constants\n",
    "    cutoff_ind = - int(np.ceil(min(0.2 * n, 3 * np.sqrt(n / Reff)))) - 1\n",
    "    cutoffmin = np.log(np.finfo(float).tiny)\n",
    "    logn = np.log(n)\n",
    "    k_min = 1/3\n",
    "\n",
    "    # loop over sets of log weights\n",
    "    for i, x in enumerate(lw_out.T if lw_out.ndim == 2 else lw_out[None, :]):\n",
    "        # improve numerical accuracy\n",
    "        x -= np.max(x)\n",
    "        # sort the array\n",
    "        x_sort_ind = np.argsort(x)\n",
    "        # divide log weights into body and right tail\n",
    "        xcutoff = max(\n",
    "            x[x_sort_ind[cutoff_ind]],\n",
    "            cutoffmin\n",
    "        )\n",
    "        expxcutoff = np.exp(xcutoff)\n",
    "        tailinds, = np.where(x > xcutoff)\n",
    "        x2 = x[tailinds]\n",
    "        n2 = len(x2)\n",
    "        if n2 <= 4:\n",
    "            # not enough tail samples for gpdfitnew\n",
    "            k = np.inf\n",
    "        else:\n",
    "            # order of tail samples\n",
    "            x2si = np.argsort(x2)\n",
    "            # fit generalized Pareto distribution to the right tail samples\n",
    "            np.exp(x2, out=x2)\n",
    "            x2 -= expxcutoff\n",
    "            k, sigma = gpdfitnew(x2, sort=x2si)\n",
    "        if k >= k_min and not np.isinf(k):\n",
    "            # no smoothing if short tail or GPD fit failed\n",
    "            # compute ordered statistic for the fit\n",
    "            sti = np.arange(0.5, n2)\n",
    "            sti /= n2\n",
    "            qq = gpinv(sti, k, sigma)\n",
    "            qq += expxcutoff\n",
    "            np.log(qq, out=qq)\n",
    "            # place the smoothed tail into the output array\n",
    "            x[tailinds[x2si]] = qq\n",
    "            # truncate smoothed values to the largest raw weight 0\n",
    "            x[x > 0] = 0\n",
    "        # renormalize weights\n",
    "        x -= sumlogs(x)\n",
    "        # store tail index k\n",
    "        kss[i] = k\n",
    "        #print(k)\n",
    "\n",
    "    # If the provided input array is one dimensional, return kss as scalar.\n",
    "    if lw_out.ndim == 1:\n",
    "        kss = kss[0]\n",
    "\n",
    "    return lw_out, kss\n",
    "\n",
    "def sumlogs(x, axis=None, out=None):\n",
    "    \"\"\"Sum of vector where numbers are represented by their logarithms.\n",
    "\n",
    "    Calculates ``np.log(np.sum(np.exp(x), axis=axis))`` in such a fashion that\n",
    "    it works even when elements have large magnitude.\n",
    "\n",
    "    \"\"\"\n",
    "    maxx = x.max(axis=axis, keepdims=True)\n",
    "    xnorm = x - maxx\n",
    "    np.exp(xnorm, out=xnorm)\n",
    "    out = np.sum(xnorm, axis=axis, out=out)\n",
    "    if isinstance(out, np.ndarray):\n",
    "        np.log(out, out=out)\n",
    "    else:\n",
    "        out = np.log(out)\n",
    "    out += np.squeeze(maxx)\n",
    "    return out\n",
    "def gpdfitnew(x, sort=True, sort_in_place=False, return_quadrature=False):\n",
    "    \"\"\"Estimate the paramaters for the Generalized Pareto Distribution (GPD)\n",
    "\n",
    "    Returns empirical Bayes estimate for the parameters of the two-parameter\n",
    "    generalized Parato distribution given the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        One dimensional data array\n",
    "\n",
    "    sort : bool or ndarray, optional\n",
    "        If known in advance, one can provide an array of indices that would\n",
    "        sort the input array `x`. If the input array is already sorted, provide\n",
    "        False. If True (default behaviour), the array is sorted internally.\n",
    "\n",
    "    sort_in_place : bool, optional\n",
    "        If `sort` is True and `sort_in_place` is True, the array is sorted\n",
    "        in-place (False by default).\n",
    "\n",
    "    return_quadrature : bool, optional\n",
    "        If True, quadrature points and weight `ks` and `w` of the marginal posterior distribution of k are also calculated and returned. False by\n",
    "        default.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    k, sigma : float\n",
    "        estimated parameter values\n",
    "\n",
    "    ks, w : ndarray\n",
    "        Quadrature points and weights of the marginal posterior distribution\n",
    "        of `k`. Returned only if `return_quadrature` is True.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function returns a negative of Zhang and Stephens's k, because it is\n",
    "    more common parameterisation.\n",
    "\n",
    "    \"\"\"\n",
    "    if x.ndim != 1 or len(x) <= 1:\n",
    "        raise ValueError(\"Invalid input array.\")\n",
    "\n",
    "    # check if x should be sorted\n",
    "    if sort is True:\n",
    "        if sort_in_place:\n",
    "            x.sort()\n",
    "            xsorted = True\n",
    "        else:\n",
    "            sort = np.argsort(x)\n",
    "            xsorted = False\n",
    "    elif sort is False:\n",
    "        xsorted = True\n",
    "    else:\n",
    "        xsorted = False\n",
    "\n",
    "    n = len(x)\n",
    "    PRIOR = 3\n",
    "    m = 30 + int(np.sqrt(n))\n",
    "\n",
    "    bs = np.arange(1, m + 1, dtype=float)\n",
    "    bs -= 0.5\n",
    "    np.divide(m, bs, out=bs)\n",
    "    np.sqrt(bs, out=bs)\n",
    "    np.subtract(1, bs, out=bs)\n",
    "    if xsorted:\n",
    "        bs /= PRIOR * x[int(n/4 + 0.5) - 1]\n",
    "        bs += 1 / x[-1]\n",
    "    else:\n",
    "        bs /= PRIOR * x[sort[int(n/4 + 0.5) - 1]]\n",
    "        bs += 1 / x[sort[-1]]\n",
    "\n",
    "    ks = np.negative(bs)\n",
    "    temp = ks[:,None] * x\n",
    "    np.log1p(temp, out=temp)\n",
    "    np.mean(temp, axis=1, out=ks)\n",
    "\n",
    "    L = bs / ks\n",
    "    np.negative(L, out=L)\n",
    "    np.log(L, out=L)\n",
    "    L -= ks\n",
    "    L -= 1\n",
    "    L *= n\n",
    "\n",
    "    temp = L - L[:,None]\n",
    "    np.exp(temp, out=temp)\n",
    "    w = np.sum(temp, axis=1)\n",
    "    np.divide(1, w, out=w)\n",
    "\n",
    "    # remove negligible weights\n",
    "    dii = w >= 10 * np.finfo(float).eps\n",
    "    if not np.all(dii):\n",
    "        w = w[dii]\n",
    "        bs = bs[dii]\n",
    "    # normalise w\n",
    "    w /= w.sum()\n",
    "\n",
    "    # posterior mean for b\n",
    "    b = np.sum(bs * w)\n",
    "    # Estimate for k, note that we return a negative of Zhang and\n",
    "    # Stephens's k, because it is more common parameterisation.\n",
    "    temp = (-b) * x\n",
    "    np.log1p(temp, out=temp)\n",
    "    k = np.mean(temp)\n",
    "    if return_quadrature:\n",
    "        np.negative(x, out=temp)\n",
    "        temp = bs[:, None] * temp\n",
    "        np.log1p(temp, out=temp)\n",
    "        ks = np.mean(temp, axis=1)\n",
    "    # estimate for sigma\n",
    "    sigma = -k / b * n / (n - 0)\n",
    "    # weakly informative prior for k\n",
    "    a = 10\n",
    "    k = k * n / (n+a) + a * 0.5 / (n+a)\n",
    "    if return_quadrature:\n",
    "        ks *= n / (n+a)\n",
    "        ks += a * 0.5 / (n+a)\n",
    "\n",
    "    if return_quadrature:\n",
    "        #print('K:'+str(k))\n",
    "        return k, sigma, ks, w\n",
    "    else:\n",
    "        #print('K:'+str(k))\n",
    "        return k, sigma\n",
    "\n",
    "def gpinv(p, k, sigma):\n",
    "    \"\"\"Inverse Generalised Pareto distribution function.\"\"\"\n",
    "    x = np.empty(p.shape)\n",
    "    x.fill(np.nan)\n",
    "    if sigma <= 0:\n",
    "        return x\n",
    "    ok = (p > 0) & (p < 1)\n",
    "    if np.all(ok):\n",
    "        if np.abs(k) < np.finfo(float).eps:\n",
    "            np.negative(p, out=x)\n",
    "            np.log1p(x, out=x)\n",
    "            np.negative(x, out=x)\n",
    "        else:\n",
    "            np.negative(p, out=x)\n",
    "            np.log1p(x, out=x)\n",
    "            x *= -k\n",
    "            np.expm1(x, out=x)\n",
    "            x /= k\n",
    "        x *= sigma\n",
    "    else:\n",
    "        if np.abs(k) < np.finfo(float).eps:\n",
    "            # x[ok] = - np.log1p(-p[ok])\n",
    "            temp = p[ok]\n",
    "            np.negative(temp, out=temp)\n",
    "            np.log1p(temp, out=temp)\n",
    "            np.negative(temp, out=temp)\n",
    "            x[ok] = temp\n",
    "        else:\n",
    "            # x[ok] = np.expm1(-k * np.log1p(-p[ok])) / k\n",
    "            temp = p[ok]\n",
    "            np.negative(temp, out=temp)\n",
    "            np.log1p(temp, out=temp)\n",
    "            temp *= -k\n",
    "            np.expm1(temp, out=temp)\n",
    "            temp /= k\n",
    "            x[ok] = temp\n",
    "        x *= sigma\n",
    "        x[p == 0] = 0\n",
    "        if k >= 0:\n",
    "            x[p == 1] = np.inf\n",
    "        else:\n",
    "            x[p == 1] = -sigma / k\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D_\\text{KL}(\\mathcal{N}_0 \\| \\mathcal{N}_1) = { 1 \\over 2 } \\left\\{ \\operatorname{tr} \\left( \\boldsymbol\\Sigma_1^{-1} \\boldsymbol\\Sigma_0 \\right) + \\left( \\boldsymbol\\mu_1 - \\boldsymbol\\mu_0\\right)^{\\rm T} \\boldsymbol\\Sigma_1^{-1} ( \\boldsymbol\\mu_1 - \\boldsymbol\\mu_0 ) - k +\\ln { |  \\boldsymbol \\Sigma_1 | \\over | \\boldsymbol\\Sigma_0 | } \\right\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample = VS[0] # Take any random Sample lets take one from our samples.\n",
    "def Var_Co(gplvm,Sample,noise_var=0.05):\n",
    "    Q = np.matmul(gplvm.base_model.kernel(Sample,gplvm.base_model.Xu).detach().numpy(),np.linalg.inv(gplvm.base_model.kernel(gplvm.base_model.Xu,gplvm.base_model.Xu).detach().numpy()))\n",
    "    Q = np.matmul(Q,gplvm.base_model.kernel(Sample,gplvm.base_model.Xu).detach().numpy().T)\n",
    "    Noise=np.eye(Q.shape[0])*np.sqrt(noise_var) \n",
    "    Noise_term = np.matmul(np.linalg.inv(Noise),np.eye(Q.shape[0]))  # Noise term\n",
    "    K_new = np.add(Q,Noise_term) #  Covariance matrix for Y\n",
    "    return K_new\n",
    "\n",
    "def Pos_Co(Y,Xs,N,D,Q,kernel,noise_var=0.05):\n",
    "    K= kernel.K(np.array(Xs),np.array(Xs)) # Covariance matrix for K(Xs,Xs), KNN\n",
    "    Noise=np.eye(K.shape[0])*np.sqrt(noise_var)  # B^-1 Noise vector\n",
    "    Noise_term = np.matmul(np.linalg.inv(Noise),np.eye(K.shape[0]))  # B^-1 I\n",
    "    K_new = np.add(K,Noise_term) #  KNN + B^(-1)I_{N}\n",
    "    return K_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_var = Var_Co(gplvm,Sample)\n",
    "K_Pos = Pos_Co(Y=Y,Xs=Sample.detach().numpy(),N=150,D=4,Q=2,kernel=kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_Diverg(mean1,mean2,K_var,K_Pos):\n",
    "    mid_term = np.subtract(mean1,mean2)\n",
    "    K_inv = np.linalg.inv(K_Pos) # Matrix \n",
    "    print(np.trace(np.matmul(K_inv,K_var)))\n",
    "    print(np.matmul(np.matmul(mid_term.T,K_inv),mid_term))\n",
    "    print(np.log(np.divide(np.linalg.det(K_Pos),np.linalg.det(K_var))))\n",
    "    return 0.5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.62632288988706\n",
      "[[16.67081723  3.26879737]\n",
      " [ 3.26879737 33.17586676]]\n",
      "-1.9054271744715265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KL_Diverg(np.array(m.X.mean),gplvm.get_param('X_loc').detach().numpy(),K_var,K_Pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-925.2973315541112,\n",
       " -932.0725197287236,\n",
       " -940.7963037223631,\n",
       " -945.3842529675011,\n",
       " -935.903447802732,\n",
       " -945.5582718762436,\n",
       " -936.5838106227084,\n",
       " -958.2299801687036,\n",
       " -944.193407649369,\n",
       " -942.4414291606791,\n",
       " -945.4721737966797,\n",
       " -955.4324014171677,\n",
       " -947.9025538320938,\n",
       " -954.3556306827629,\n",
       " -951.3335378073441,\n",
       " -935.2073013339794,\n",
       " -935.1000973822136,\n",
       " -964.7890849831392,\n",
       " -957.7606323533887,\n",
       " -945.0188094039634,\n",
       " -928.0878157685709,\n",
       " -944.5483148684995,\n",
       " -949.2980239904329,\n",
       " -946.6947793912094,\n",
       " -952.2968603879907,\n",
       " -942.9422673073642,\n",
       " -931.3150061685176,\n",
       " -947.3298264872111,\n",
       " -958.3790150060113,\n",
       " -951.0933767710188,\n",
       " -946.725295166102,\n",
       " -937.3287043069136,\n",
       " -940.4527983373575,\n",
       " -946.9621718226612,\n",
       " -962.2260664341122,\n",
       " -940.0093744040944,\n",
       " -942.9003340160855,\n",
       " -941.1736998256686,\n",
       " -951.972667132275,\n",
       " -939.3235549879613,\n",
       " -934.4579195627168,\n",
       " -944.8126799646169,\n",
       " -948.0063838186793,\n",
       " -951.0411237418028,\n",
       " -962.9065750842063,\n",
       " -961.9851863285793,\n",
       " -950.1013060859344,\n",
       " -948.377031096716,\n",
       " -960.088709508857,\n",
       " -957.6989289530501,\n",
       " -940.1486689601127,\n",
       " -946.5433842342816,\n",
       " -955.8398958916662,\n",
       " -951.1965176207967,\n",
       " -939.1681510680968,\n",
       " -939.6207717194667,\n",
       " -946.7920969462033,\n",
       " -934.8392468600409,\n",
       " -945.9687648961523,\n",
       " -956.339548366713,\n",
       " -950.8175672493624,\n",
       " -948.5836951289219,\n",
       " -949.2475066537362,\n",
       " -931.1318713979135,\n",
       " -952.3772315542476,\n",
       " -949.9853578559135,\n",
       " -950.2739044597873,\n",
       " -948.0645301384741,\n",
       " -947.3751340062576,\n",
       " -948.8317473004743,\n",
       " -940.9126437368166,\n",
       " -934.8452646903413,\n",
       " -948.1693656262096,\n",
       " -941.1698324419195,\n",
       " -940.1019000178137,\n",
       " -940.3484676325272,\n",
       " -930.5370089470343,\n",
       " -930.8946496094617,\n",
       " -944.6311944461512,\n",
       " -951.022427430772,\n",
       " -958.0128473219056,\n",
       " -937.1188046053368,\n",
       " -958.21449311656,\n",
       " -949.7088435787424,\n",
       " -949.2656490613786,\n",
       " -939.7104735221333,\n",
       " -945.560906112842,\n",
       " -954.9702323802835,\n",
       " -944.5489365632011,\n",
       " -941.2010895285877,\n",
       " -944.0403013202964,\n",
       " -951.539777641268,\n",
       " -940.3594963092684,\n",
       " -960.6730314625331,\n",
       " -954.2004628071102,\n",
       " -939.964970152191,\n",
       " -946.7954231944269,\n",
       " -947.1065121100207,\n",
       " -937.4098061652726,\n",
       " -945.9072024135617,\n",
       " -942.9041072865417,\n",
       " -961.9043216792419,\n",
       " -962.3861335937572,\n",
       " -936.2805954456303,\n",
       " -942.7758613151166,\n",
       " -951.6320369213327,\n",
       " -943.3073909796108,\n",
       " -947.8339441878657,\n",
       " -945.9728757711973,\n",
       " -932.8177453621634,\n",
       " -935.6900141134038,\n",
       " -955.8044212849758,\n",
       " -945.4029838494331,\n",
       " -949.2482066202087,\n",
       " -951.7651386816469,\n",
       " -958.5552635470026,\n",
       " -956.6958961406393,\n",
       " -953.4110678357418,\n",
       " -956.6107201921623,\n",
       " -947.9839102916185,\n",
       " -932.9811238735191,\n",
       " -960.9228457798469,\n",
       " -949.2484699099548,\n",
       " -963.6561618735756,\n",
       " -946.0651995983925,\n",
       " -956.4233447523303,\n",
       " -942.0055458359776,\n",
       " -949.070590282199,\n",
       " -936.9549977742568,\n",
       " -939.0215755536714,\n",
       " -949.999093938613,\n",
       " -952.3859053560851,\n",
       " -937.4596962849523,\n",
       " -954.3879095010823,\n",
       " -937.3824584633578,\n",
       " -952.3488953408474,\n",
       " -954.2595030006366,\n",
       " -965.8247631688595,\n",
       " -964.3861583575996,\n",
       " -952.9032954504713,\n",
       " -954.7576248125944,\n",
       " -962.5525168139073,\n",
       " -959.0947904726527,\n",
       " -957.8890739390135,\n",
       " -941.0234892948232,\n",
       " -960.8987855589916,\n",
       " -948.1911492638388,\n",
       " -949.8424566000273,\n",
       " -950.9991751445949,\n",
       " -950.9286165013868,\n",
       " -946.6987548387937,\n",
       " -951.1698760110995,\n",
       " -949.9609149499502,\n",
       " -951.03842948642,\n",
       " -950.0895242610119,\n",
       " -933.373504066096,\n",
       " -954.9671947318983,\n",
       " -925.5711666669882,\n",
       " -961.5143217499638,\n",
       " -943.0108359554284,\n",
       " -944.7809493280017,\n",
       " -931.9634506911286,\n",
       " -955.3593311609739,\n",
       " -928.5011077036017,\n",
       " -955.7406937390865,\n",
       " -949.4312728512986,\n",
       " -953.6445857927441,\n",
       " -940.9933590225235,\n",
       " -932.0362014318014,\n",
       " -934.8082345430478,\n",
       " -948.1836500860607,\n",
       " -954.9670403629198,\n",
       " -959.61465162222,\n",
       " -943.985058018799,\n",
       " -955.8581760334401,\n",
       " -947.3950556076556,\n",
       " -959.5444196906026,\n",
       " -942.5720270729535,\n",
       " -944.8977102069707,\n",
       " -944.4755716474268,\n",
       " -949.8133780477591,\n",
       " -950.1018714935531,\n",
       " -947.0314832320591,\n",
       " -943.2105634077636,\n",
       " -970.2861355882757,\n",
       " -934.915580797715,\n",
       " -954.3862215952769,\n",
       " -940.5231051997599,\n",
       " -969.6328764747219,\n",
       " -937.9693964070509,\n",
       " -962.2565261048335,\n",
       " -939.0539851326371,\n",
       " -952.2753983399865,\n",
       " -946.5696307531257,\n",
       " -955.6883368563199,\n",
       " -945.2534082812707,\n",
       " -951.2634356550359,\n",
       " -952.8544431331063,\n",
       " -939.2873530995734,\n",
       " -944.1766081265391,\n",
       " -947.6192177910931,\n",
       " -950.397821041246,\n",
       " -972.1349454731554,\n",
       " -955.3526764127149,\n",
       " -940.1449096083786,\n",
       " -945.1037892798914,\n",
       " -944.1896002958111,\n",
       " -964.0354460076442,\n",
       " -933.7175363721218,\n",
       " -944.2914778046064,\n",
       " -948.3389565596998,\n",
       " -941.7157092581732,\n",
       " -957.7051535332046,\n",
       " -955.378062040538,\n",
       " -953.3085061376046,\n",
       " -942.4540372330542,\n",
       " -937.4823458526835,\n",
       " -942.3368938432153,\n",
       " -946.9881092095579,\n",
       " -955.4424409740911,\n",
       " -945.1756136523065,\n",
       " -936.0772721366395,\n",
       " -937.2446328750551,\n",
       " -937.6603781213136,\n",
       " -959.3096295846203,\n",
       " -942.5881919588287,\n",
       " -935.4077890497189,\n",
       " -953.4137564536834,\n",
       " -959.4429164429976,\n",
       " -945.3353901360294,\n",
       " -949.0348750889592,\n",
       " -965.961514085095,\n",
       " -951.0179182376241,\n",
       " -959.3717709494072,\n",
       " -953.1981785406075,\n",
       " -942.8800186165622,\n",
       " -947.463608167695,\n",
       " -949.2247758608659,\n",
       " -933.2158069834647,\n",
       " -946.0514381171415,\n",
       " -951.0908969203116,\n",
       " -947.0153206340163,\n",
       " -939.0731420887387,\n",
       " -943.1474323293389,\n",
       " -952.6007677403716,\n",
       " -962.3915906402901,\n",
       " -938.4198175691097,\n",
       " -953.9227807420198,\n",
       " -936.5761653537003,\n",
       " -950.0444431473315,\n",
       " -940.4972523556332,\n",
       " -950.3414302267657,\n",
       " -948.9018396695624,\n",
       " -943.1810415830201,\n",
       " -956.5642244584276,\n",
       " -951.1293146201717,\n",
       " -942.2849033996259,\n",
       " -961.907313282708,\n",
       " -951.6084638385109,\n",
       " -949.0055570991962,\n",
       " -953.577402503387,\n",
       " -938.0393484213505,\n",
       " -934.8720536837627,\n",
       " -947.8753602107219,\n",
       " -949.8791785634696,\n",
       " -960.2060946577141,\n",
       " -941.4924952258541,\n",
       " -949.619797868359,\n",
       " -954.1743718237938,\n",
       " -949.2250963758613,\n",
       " -942.577060839675,\n",
       " -972.3836628586017,\n",
       " -960.584143430288,\n",
       " -951.7103836786423,\n",
       " -941.3497860079159,\n",
       " -944.9426667744791,\n",
       " -949.4140056765126,\n",
       " -937.7385414937662,\n",
       " -953.1117122069264,\n",
       " -953.3949441802296,\n",
       " -940.4290538285994,\n",
       " -955.4238242424213,\n",
       " -939.5108555907422,\n",
       " -959.6619575297664,\n",
       " -953.2767685806836,\n",
       " -956.6436268858591,\n",
       " -943.6465680706867,\n",
       " -967.6506251838248,\n",
       " -970.8039300081341,\n",
       " -944.7272616613923,\n",
       " -938.0124345139141,\n",
       " -963.9900585633035,\n",
       " -937.3502261554822,\n",
       " -925.8620002334513,\n",
       " -937.2397293970715,\n",
       " -943.7194081946717,\n",
       " -946.9595994204403,\n",
       " -953.3010704072298,\n",
       " -937.813331844415,\n",
       " -946.7847269303669,\n",
       " -940.6675477163647,\n",
       " -956.9460361149432,\n",
       " -943.4068239025094,\n",
       " -948.7082771354333,\n",
       " -947.3621703032352,\n",
       " -945.1327009086717,\n",
       " -960.694027263222,\n",
       " -946.9406149596227,\n",
       " -952.8366683760919,\n",
       " -942.385401660731,\n",
       " -943.3456229941024,\n",
       " -943.3023063889748,\n",
       " -937.049840769918,\n",
       " -942.5344711411152,\n",
       " -953.5615587483547,\n",
       " -940.7974524499597,\n",
       " -957.6034490392102,\n",
       " -945.5360175546514,\n",
       " -936.9412465477529,\n",
       " -946.0350363727621,\n",
       " -945.2321049987536,\n",
       " -945.3376566778413,\n",
       " -949.6783756754747,\n",
       " -947.5063692693116,\n",
       " -936.5471205516615,\n",
       " -960.0039792755047,\n",
       " -957.0288147491501,\n",
       " -943.4100336806628,\n",
       " -968.4327781335572,\n",
       " -947.6070551669291,\n",
       " -952.0954329204554,\n",
       " -942.0546401402501,\n",
       " -942.6830271598171,\n",
       " -950.4046213622303,\n",
       " -960.4146297945928,\n",
       " -938.7544573003084,\n",
       " -948.390791286834,\n",
       " -962.1386568883366,\n",
       " -946.114876929708,\n",
       " -963.7960335597487,\n",
       " -935.8074843744091,\n",
       " -927.0041299161726,\n",
       " -952.2501206081807,\n",
       " -944.8738879326942,\n",
       " -935.2211398316363,\n",
       " -949.6879802543451,\n",
       " -963.6951492753664,\n",
       " -952.2338542281364,\n",
       " -948.5761244829971,\n",
       " -935.1182378587984,\n",
       " -939.2693256889706,\n",
       " -973.273927685169,\n",
       " -945.9863063943129,\n",
       " -946.2983353478585,\n",
       " -955.8764950563467,\n",
       " -945.2763715983315,\n",
       " -942.6466602199749,\n",
       " -950.1007346592711,\n",
       " -938.0263369084284,\n",
       " -937.8283854449992,\n",
       " -959.4583735907535,\n",
       " -943.6800539479669,\n",
       " -934.0765505748623,\n",
       " -945.4448477671308,\n",
       " -955.7298378236395,\n",
       " -947.7783562783678,\n",
       " -947.7468824628536,\n",
       " -939.600959767906,\n",
       " -934.586246730723,\n",
       " -946.3216147123114,\n",
       " -960.7205272025622,\n",
       " -946.940114548992,\n",
       " -942.188497094558,\n",
       " -935.1944428928472,\n",
       " -957.3719664975775,\n",
       " -952.9504074263641,\n",
       " -941.4273476260083,\n",
       " -941.8744375943381,\n",
       " -929.8325693654808,\n",
       " -937.950152275584,\n",
       " -948.7489270707599,\n",
       " -942.233401317373,\n",
       " -966.4214645261168,\n",
       " -931.3421557449375,\n",
       " -947.0984697881183,\n",
       " -938.3201534077432,\n",
       " -949.8137904533502,\n",
       " -955.1091361075025,\n",
       " -942.0498043372659,\n",
       " -954.1562062946855,\n",
       " -955.0384913022398,\n",
       " -945.9704294008567,\n",
       " -939.9139741548573,\n",
       " -934.6082197041721,\n",
       " -945.9456496573829,\n",
       " -965.9114416243261,\n",
       " -938.320604465461,\n",
       " -954.0647098324343,\n",
       " -944.562142259141,\n",
       " -956.4540699710807,\n",
       " -952.170789217685,\n",
       " -941.6042512442937,\n",
       " -943.8116904776938,\n",
       " -934.1176304736242,\n",
       " -938.8146565820832,\n",
       " -970.0326033033189,\n",
       " -957.5155753174577,\n",
       " -956.2720385693046,\n",
       " -944.732438686523,\n",
       " -948.5012319986984,\n",
       " -945.718203134432,\n",
       " -944.1769026258612,\n",
       " -941.4386295006773,\n",
       " -964.4282394648649,\n",
       " -949.6270251013347,\n",
       " -946.3007429650369,\n",
       " -943.5438805406768,\n",
       " -955.1250866036224,\n",
       " -956.3475785538966,\n",
       " -935.4201242505817,\n",
       " -953.1779435236841,\n",
       " -958.561808737993,\n",
       " -945.5472565563402,\n",
       " -961.7265731317062,\n",
       " -942.8169598251382,\n",
       " -947.1080825032099,\n",
       " -938.7284089779467,\n",
       " -950.9107779562198,\n",
       " -948.1007607625182,\n",
       " -957.0444649104217,\n",
       " -940.8526976784913,\n",
       " -961.0652127809026,\n",
       " -947.5098316014618,\n",
       " -953.142563653777,\n",
       " -937.5998475803971,\n",
       " -937.8145878273594,\n",
       " -965.3883358635463,\n",
       " -959.3854574013671,\n",
       " -947.374176012936,\n",
       " -947.0357691695381,\n",
       " -926.9380740171044,\n",
       " -943.3817954398337,\n",
       " -938.8984095670464,\n",
       " -950.4814029065541,\n",
       " -954.1501536895528,\n",
       " -937.4335243252749,\n",
       " -943.0835798447054,\n",
       " -938.6154618780303,\n",
       " -947.1735731216403,\n",
       " -951.4441024066731,\n",
       " -948.9386871625919,\n",
       " -941.6623506410335,\n",
       " -956.4644014699359,\n",
       " -946.1679711102103,\n",
       " -953.432494065033,\n",
       " -944.5261588245262,\n",
       " -950.1059977928114,\n",
       " -950.7769804274019,\n",
       " -960.7878103396805,\n",
       " -947.599903344991,\n",
       " -947.0288764328957,\n",
       " -941.0838755956713,\n",
       " -951.019257767208,\n",
       " -950.8755077287007,\n",
       " -963.0261400191188,\n",
       " -932.8106421938076,\n",
       " -943.5256885724499,\n",
       " -940.7062129929036,\n",
       " -955.9106914472488,\n",
       " -949.9883572098042,\n",
       " -953.3687299862546,\n",
       " -954.0238087948557,\n",
       " -939.1952476772526,\n",
       " -947.4235386833982,\n",
       " -935.0182269771493,\n",
       " -963.5600788689363,\n",
       " -959.6109042120248,\n",
       " -934.7416257074176,\n",
       " -943.8316717735557,\n",
       " -944.1687882429118,\n",
       " -945.3386189592845,\n",
       " -950.4676663654492,\n",
       " -947.8420744982196,\n",
       " -932.6604624162882,\n",
       " -954.5507005923919,\n",
       " -953.9722423809638,\n",
       " -940.6593719922383,\n",
       " -945.1482045735806,\n",
       " -946.9503197035447,\n",
       " -955.0693793665191,\n",
       " -967.3634810662618,\n",
       " -949.8361125118975,\n",
       " -944.8511698951368,\n",
       " -936.1411563386555,\n",
       " -956.082443808408,\n",
       " -947.4614816232872,\n",
       " -941.8752635969432,\n",
       " -947.496685392947,\n",
       " -946.6268358380192,\n",
       " -939.9615597252059]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
