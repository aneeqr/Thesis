{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import tensorflow as tf\n",
    "from edward.models import (\n",
    "\tBeta,\n",
    "\tBernoulli,\n",
    "\tEmpirical\n",
    ")\n",
    "from edward import models as mod\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "import pyro.distributions as dist\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import sympy as sp\n",
    "#import pymc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "from scipy.special import gamma\n",
    "\n",
    "from sympy.interactive import printing\n",
    "printing.init_printing()\n",
    "torch.manual_seed(101)\n",
    "pyro.set_rng_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for _ in range(6):\n",
    "    data.append(torch.tensor(1.0))\n",
    "for _ in range(4):\n",
    "    data.append(torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_numpy=[]\n",
    "for i in data:\n",
    "    data_numpy.append(i.detach().numpy())\n",
    "data_numpy=np.array(data_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior Mean\n",
    "a1=10\n",
    "a2=10\n",
    "prior_mean = a1 / (a1 + a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior Mean (Analytic): 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "a1_hat = a1 + data_numpy.sum()\n",
    "a2_hat = a2 + len(data) - data_numpy.sum()\n",
    "\n",
    "# Posterior Mean\n",
    "post_mean = a1_hat / (a1_hat + a2_hat)\n",
    "print('Posterior Mean (Analytic):', post_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................."
     ]
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "n_steps = 2 if smoke_test else 2000\n",
    "\n",
    "# enable validation (e.g. validate parameters of distributions)\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "# clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()\n",
    "def model(data):\n",
    "    # define the hyperparameters that control the beta prior\n",
    "    alpha0 = torch.tensor(10.0)\n",
    "    beta0 = torch.tensor(10.0)\n",
    "    # sample f from the beta prior\n",
    "    f = pyro.sample(\"latent_fairness\", dist.Beta(alpha0, beta0))\n",
    "    # loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli likelihood\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
    "\n",
    "def guide(data):\n",
    "    # register the two variational parameters with Pyro\n",
    "    # - both parameters will have initial value 15.0.\n",
    "    # - because we invoke constraints.positive, the optimizer\n",
    "    # will take gradients on the unconstrained parameters\n",
    "    # (which are related to the constrained parameters by a log)\n",
    "    alpha_q = pyro.param(\"alpha_q\", torch.tensor(15.0),\n",
    "                         constraint=constraints.positive)\n",
    "    beta_q = pyro.param(\"beta_q\", torch.tensor(15.0),\n",
    "                        constraint=constraints.positive)\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n",
    "\n",
    "# setup the optimizer\n",
    "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "# do gradient steps\n",
    "for step in range(n_steps):\n",
    "    svi.step(data)\n",
    "    if step % 100 == 0:\n",
    "        print('.', end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "based on the data and our prior belief, the fairness of the coin is 0.529 +- 0.090\n"
     ]
    }
   ],
   "source": [
    "# grab the learned variational parameters\n",
    "alpha_q = pyro.param(\"alpha_q\").item()\n",
    "beta_q = pyro.param(\"beta_q\").item()\n",
    "\n",
    "# here we use some facts about the beta distribution\n",
    "# compute the inferred mean of the coin's fairness\n",
    "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "# compute inferred standard deviation\n",
    "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "inferred_std = inferred_mean * math.sqrt(factor)\n",
    "\n",
    "print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational Distribution\n",
    "Samples = 1000\n",
    "q=[] \n",
    "for i in range(Samples):\n",
    "    q.append(pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q)).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pos = stats.beta(a1_hat,a2_hat).pdf(q)\n",
    "q=stats.beta(alpha_q,beta_q).pdf(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ratio = []\n",
    "ind = 0\n",
    "for i in Pos:\n",
    "    Ratio.append(np.log(i/q[ind]))\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:0.1026132526492902\n"
     ]
    }
   ],
   "source": [
    "print('K:'+str(psisloo(-1*np.array(Ratio))[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$$2.575393367083927$$"
      ],
      "text/plain": [
       "2.575393367083927"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 0.5*(KL(Pos,q)+KL(q,Pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Beta(10., 10.)\n",
    "x = Bernoulli(probs=p, sample_shape=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aneeqr/.local/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 4s | Acceptance Rate: 0.806\n"
     ]
    }
   ],
   "source": [
    "qp = Empirical(params=tf.get_variable(\"qp/params\", [1000], initializer=tf.constant_initializer(0.5)))\n",
    "proposal_p = Beta(15.0, 15.0)\n",
    "inference = ed.MetropolisHastings({p: qp}, {p: proposal_p}, data={x: data_numpy})\n",
    "inference.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred posterior mean:\n",
      "0.53722775\n",
      "Inferred posterior stddev:\n",
      "0.088133216\n"
     ]
    }
   ],
   "source": [
    "sess = ed.get_session()\n",
    "mean, stddev = sess.run([qp.mean(), qp.stddev()])\n",
    "print(\"Inferred posterior mean:\")\n",
    "print(mean)\n",
    "print(\"Inferred posterior stddev:\")\n",
    "print(stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "q= sess.run(qp.sample(sample_shape=1000,seed=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=16.702\n",
    "b1=13.694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pos = stats.beta(a1_hat,a2_hat).pdf(q)\n",
    "q= stats.beta(a1,b1).pdf(q)\n",
    "Ratio = []\n",
    "ind = 0\n",
    "for i in Pos:\n",
    "    Ratio.append(np.log(i/q[ind]))\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:0.30463627648751396\n"
     ]
    }
   ],
   "source": [
    "print('K:'+str(psisloo(1*np.array(Ratio))[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$$36.42701074217375$$"
      ],
      "text/plain": [
       "36.42701074217375"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5*(KL(Pos,q)+KL(q,Pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(P,Q):\n",
    "    epsilon = 0.00001\n",
    "\n",
    "    # You may want to instead make copies to avoid changing the np arrays.\n",
    "    P = P+epsilon\n",
    "    Q = Q+epsilon\n",
    "\n",
    "    divergence = np.sum(P*np.log2(P/Q))\n",
    "    return divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psisloo(log_lik, **kwargs):\n",
    "    r\"\"\"PSIS leave-one-out log predictive densities.\n",
    "\n",
    "    Computes the log predictive densities given posterior samples of the log\n",
    "    likelihood terms :math:`p(y_i|\\theta^s)` in input parameter `log_lik`.\n",
    "    Returns a sum of the leave-one-out log predictive densities `loo`,\n",
    "    individual leave-one-out log predictive density terms `loos` and an estimate\n",
    "    of Pareto tail indeces `ks`. The estimates are unreliable if tail index\n",
    "    ``k > 0.7`` (see more in the references listed in the module docstring).\n",
    "\n",
    "    Additional keyword arguments are passed to the :meth:`psislw()` function\n",
    "    (see the corresponding documentation).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_lik : ndarray\n",
    "        Array of size n x m containing n posterior samples of the log likelihood\n",
    "        terms :math:`p(y_i|\\theta^s)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loo : scalar\n",
    "        sum of the leave-one-out log predictive densities\n",
    "\n",
    "    loos : ndarray\n",
    "        individual leave-one-out log predictive density terms\n",
    "\n",
    "    ks : ndarray\n",
    "        estimated Pareto tail indeces\n",
    "\n",
    "    \"\"\"\n",
    "    # ensure overwrite flag in passed arguments\n",
    "    kwargs['overwrite_lw'] = True\n",
    "    # log raw weights from log_lik\n",
    "    lw = -log_lik\n",
    "    # compute Pareto smoothed log weights given raw log weights\n",
    "    lw, ks = psislw(lw, **kwargs)\n",
    "    # compute\n",
    "    lw += log_lik\n",
    "    loos = sumlogs(lw, axis=0)\n",
    "    loo = loos.sum()\n",
    "    #print('Ks'+str(ks))\n",
    "    return loo, loos, ks\n",
    "\n",
    "def psislw(lw, Reff=1.0, overwrite_lw=False):\n",
    "    \"\"\"Pareto smoothed importance sampling (PSIS).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lw : ndarray\n",
    "        Array of size n x m containing m sets of n log weights. It is also\n",
    "        possible to provide one dimensional array of length n.\n",
    "\n",
    "    Reff : scalar, optional\n",
    "        relative MCMC efficiency ``N_eff / N``\n",
    "\n",
    "    overwrite_lw : bool, optional\n",
    "        If True, the input array `lw` is smoothed in-place, assuming the array\n",
    "        is F-contiguous. By default, a new array is allocated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lw_out : ndarray\n",
    "        smoothed log weights\n",
    "    kss : ndarray\n",
    "        Pareto tail indices\n",
    "\n",
    "    \"\"\"\n",
    "    if lw.ndim == 2:\n",
    "        n, m = lw.shape\n",
    "    elif lw.ndim == 1:\n",
    "        n = len(lw)\n",
    "        m = 1\n",
    "    else:\n",
    "        raise ValueError(\"Argument `lw` must be 1 or 2 dimensional.\")\n",
    "    if n <= 1:\n",
    "        raise ValueError(\"More than one log-weight needed.\")\n",
    "\n",
    "    if overwrite_lw and lw.flags.f_contiguous:\n",
    "        # in-place operation\n",
    "        lw_out = lw\n",
    "    else:\n",
    "        # allocate new array for output\n",
    "        lw_out = np.copy(lw, order='F')\n",
    "\n",
    "    # allocate output array for kss\n",
    "    kss = np.empty(m)\n",
    "    #print(kss)\n",
    "\n",
    "    # precalculate constants\n",
    "    cutoff_ind = - int(np.ceil(min(0.2 * n, 3 * np.sqrt(n / Reff)))) - 1\n",
    "    cutoffmin = np.log(np.finfo(float).tiny)\n",
    "    logn = np.log(n)\n",
    "    k_min = 1/3\n",
    "\n",
    "    # loop over sets of log weights\n",
    "    for i, x in enumerate(lw_out.T if lw_out.ndim == 2 else lw_out[None, :]):\n",
    "        # improve numerical accuracy\n",
    "        x -= np.max(x)\n",
    "        # sort the array\n",
    "        x_sort_ind = np.argsort(x)\n",
    "        # divide log weights into body and right tail\n",
    "        xcutoff = max(\n",
    "            x[x_sort_ind[cutoff_ind]],\n",
    "            cutoffmin\n",
    "        )\n",
    "        expxcutoff = np.exp(xcutoff)\n",
    "        tailinds, = np.where(x > xcutoff)\n",
    "        x2 = x[tailinds]\n",
    "        n2 = len(x2)\n",
    "        if n2 <= 4:\n",
    "            # not enough tail samples for gpdfitnew\n",
    "            k = np.inf\n",
    "        else:\n",
    "            # order of tail samples\n",
    "            x2si = np.argsort(x2)\n",
    "            # fit generalized Pareto distribution to the right tail samples\n",
    "            np.exp(x2, out=x2)\n",
    "            x2 -= expxcutoff\n",
    "            k, sigma = gpdfitnew(x2, sort=x2si)\n",
    "        if k >= k_min and not np.isinf(k):\n",
    "            # no smoothing if short tail or GPD fit failed\n",
    "            # compute ordered statistic for the fit\n",
    "            sti = np.arange(0.5, n2)\n",
    "            sti /= n2\n",
    "            qq = gpinv(sti, k, sigma)\n",
    "            qq += expxcutoff\n",
    "            np.log(qq, out=qq)\n",
    "            # place the smoothed tail into the output array\n",
    "            x[tailinds[x2si]] = qq\n",
    "            # truncate smoothed values to the largest raw weight 0\n",
    "            x[x > 0] = 0\n",
    "        # renormalize weights\n",
    "        x -= sumlogs(x)\n",
    "        # store tail index k\n",
    "        kss[i] = k\n",
    "        #print(k)\n",
    "\n",
    "    # If the provided input array is one dimensional, return kss as scalar.\n",
    "    if lw_out.ndim == 1:\n",
    "        kss = kss[0]\n",
    "\n",
    "    return lw_out, kss\n",
    "\n",
    "def sumlogs(x, axis=None, out=None):\n",
    "    \"\"\"Sum of vector where numbers are represented by their logarithms.\n",
    "\n",
    "    Calculates ``np.log(np.sum(np.exp(x), axis=axis))`` in such a fashion that\n",
    "    it works even when elements have large magnitude.\n",
    "\n",
    "    \"\"\"\n",
    "    maxx = x.max(axis=axis, keepdims=True)\n",
    "    xnorm = x - maxx\n",
    "    np.exp(xnorm, out=xnorm)\n",
    "    out = np.sum(xnorm, axis=axis, out=out)\n",
    "    if isinstance(out, np.ndarray):\n",
    "        np.log(out, out=out)\n",
    "    else:\n",
    "        out = np.log(out)\n",
    "    out += np.squeeze(maxx)\n",
    "    return out\n",
    "def gpdfitnew(x, sort=True, sort_in_place=False, return_quadrature=False):\n",
    "    \"\"\"Estimate the paramaters for the Generalized Pareto Distribution (GPD)\n",
    "\n",
    "    Returns empirical Bayes estimate for the parameters of the two-parameter\n",
    "    generalized Parato distribution given the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        One dimensional data array\n",
    "\n",
    "    sort : bool or ndarray, optional\n",
    "        If known in advance, one can provide an array of indices that would\n",
    "        sort the input array `x`. If the input array is already sorted, provide\n",
    "        False. If True (default behaviour), the array is sorted internally.\n",
    "\n",
    "    sort_in_place : bool, optional\n",
    "        If `sort` is True and `sort_in_place` is True, the array is sorted\n",
    "        in-place (False by default).\n",
    "\n",
    "    return_quadrature : bool, optional\n",
    "        If True, quadrature points and weight `ks` and `w` of the marginal posterior distribution of k are also calculated and returned. False by\n",
    "        default.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    k, sigma : float\n",
    "        estimated parameter values\n",
    "\n",
    "    ks, w : ndarray\n",
    "        Quadrature points and weights of the marginal posterior distribution\n",
    "        of `k`. Returned only if `return_quadrature` is True.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function returns a negative of Zhang and Stephens's k, because it is\n",
    "    more common parameterisation.\n",
    "\n",
    "    \"\"\"\n",
    "    if x.ndim != 1 or len(x) <= 1:\n",
    "        raise ValueError(\"Invalid input array.\")\n",
    "\n",
    "    # check if x should be sorted\n",
    "    if sort is True:\n",
    "        if sort_in_place:\n",
    "            x.sort()\n",
    "            xsorted = True\n",
    "        else:\n",
    "            sort = np.argsort(x)\n",
    "            xsorted = False\n",
    "    elif sort is False:\n",
    "        xsorted = True\n",
    "    else:\n",
    "        xsorted = False\n",
    "\n",
    "    n = len(x)\n",
    "    PRIOR = 3\n",
    "    m = 30 + int(np.sqrt(n))\n",
    "\n",
    "    bs = np.arange(1, m + 1, dtype=float)\n",
    "    bs -= 0.5\n",
    "    np.divide(m, bs, out=bs)\n",
    "    np.sqrt(bs, out=bs)\n",
    "    np.subtract(1, bs, out=bs)\n",
    "    if xsorted:\n",
    "        bs /= PRIOR * x[int(n/4 + 0.5) - 1]\n",
    "        bs += 1 / x[-1]\n",
    "    else:\n",
    "        bs /= PRIOR * x[sort[int(n/4 + 0.5) - 1]]\n",
    "        bs += 1 / x[sort[-1]]\n",
    "\n",
    "    ks = np.negative(bs)\n",
    "    temp = ks[:,None] * x\n",
    "    np.log1p(temp, out=temp)\n",
    "    np.mean(temp, axis=1, out=ks)\n",
    "\n",
    "    L = bs / ks\n",
    "    np.negative(L, out=L)\n",
    "    np.log(L, out=L)\n",
    "    L -= ks\n",
    "    L -= 1\n",
    "    L *= n\n",
    "\n",
    "    temp = L - L[:,None]\n",
    "    np.exp(temp, out=temp)\n",
    "    w = np.sum(temp, axis=1)\n",
    "    np.divide(1, w, out=w)\n",
    "\n",
    "    # remove negligible weights\n",
    "    dii = w >= 10 * np.finfo(float).eps\n",
    "    if not np.all(dii):\n",
    "        w = w[dii]\n",
    "        bs = bs[dii]\n",
    "    # normalise w\n",
    "    w /= w.sum()\n",
    "\n",
    "    # posterior mean for b\n",
    "    b = np.sum(bs * w)\n",
    "    # Estimate for k, note that we return a negative of Zhang and\n",
    "    # Stephens's k, because it is more common parameterisation.\n",
    "    temp = (-b) * x\n",
    "    np.log1p(temp, out=temp)\n",
    "    k = np.mean(temp)\n",
    "    if return_quadrature:\n",
    "        np.negative(x, out=temp)\n",
    "        temp = bs[:, None] * temp\n",
    "        np.log1p(temp, out=temp)\n",
    "        ks = np.mean(temp, axis=1)\n",
    "    # estimate for sigma\n",
    "    sigma = -k / b * n / (n - 0)\n",
    "    # weakly informative prior for k\n",
    "    a = 10\n",
    "    k = k * n / (n+a) + a * 0.5 / (n+a)\n",
    "    if return_quadrature:\n",
    "        ks *= n / (n+a)\n",
    "        ks += a * 0.5 / (n+a)\n",
    "\n",
    "    if return_quadrature:\n",
    "        #print('K:'+str(k))\n",
    "        return k, sigma, ks, w\n",
    "    else:\n",
    "        #print('K:'+str(k))\n",
    "        return k, sigma\n",
    "\n",
    "def gpinv(p, k, sigma):\n",
    "    \"\"\"Inverse Generalised Pareto distribution function.\"\"\"\n",
    "    x = np.empty(p.shape)\n",
    "    x.fill(np.nan)\n",
    "    if sigma <= 0:\n",
    "        return x\n",
    "    ok = (p > 0) & (p < 1)\n",
    "    if np.all(ok):\n",
    "        if np.abs(k) < np.finfo(float).eps:\n",
    "            np.negative(p, out=x)\n",
    "            np.log1p(x, out=x)\n",
    "            np.negative(x, out=x)\n",
    "        else:\n",
    "            np.negative(p, out=x)\n",
    "            np.log1p(x, out=x)\n",
    "            x *= -k\n",
    "            np.expm1(x, out=x)\n",
    "            x /= k\n",
    "        x *= sigma\n",
    "    else:\n",
    "        if np.abs(k) < np.finfo(float).eps:\n",
    "            # x[ok] = - np.log1p(-p[ok])\n",
    "            temp = p[ok]\n",
    "            np.negative(temp, out=temp)\n",
    "            np.log1p(temp, out=temp)\n",
    "            np.negative(temp, out=temp)\n",
    "            x[ok] = temp\n",
    "        else:\n",
    "            # x[ok] = np.expm1(-k * np.log1p(-p[ok])) / k\n",
    "            temp = p[ok]\n",
    "            np.negative(temp, out=temp)\n",
    "            np.log1p(temp, out=temp)\n",
    "            temp *= -k\n",
    "            np.expm1(temp, out=temp)\n",
    "            temp /= k\n",
    "            x[ok] = temp\n",
    "        x *= sigma\n",
    "        x[p == 0] = 0\n",
    "        if k >= 0:\n",
    "            x[p == 1] = np.inf\n",
    "        else:\n",
    "            x[p == 1] = -sigma / k\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
