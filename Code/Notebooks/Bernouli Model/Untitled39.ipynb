{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import edward as ed\n",
    "import tensorflow as tf\n",
    "from edward.models import (\n",
    "\tBeta,\n",
    "\tBernoulli,\n",
    "\tEmpirical\n",
    ")\n",
    "from edward import models as mod\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "import pyro.distributions as dist\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import sympy as sp\n",
    "#import pymc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import stats\n",
    "from scipy.special import gamma\n",
    "\n",
    "from sympy.interactive import printing\n",
    "printing.init_printing()\n",
    "torch.manual_seed(101)\n",
    "pyro.set_rng_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/1000 [  0%]                                ETA: 39661s | Loss: 7.280=\n",
      "  50/1000 [  5%] █                              ETA: 758s | Loss: 7.587  =\n",
      " 100/1000 [ 10%] ███                            ETA: 360s | Loss: 7.366=\n",
      " 150/1000 [ 15%] ████                           ETA: 227s | Loss: 7.090=\n",
      " 200/1000 [ 20%] ██████                         ETA: 160s | Loss: 7.242=\n",
      " 250/1000 [ 25%] ███████                        ETA: 121s | Loss: 7.700=\n",
      " 300/1000 [ 30%] █████████                      ETA: 94s | Loss: 6.825 =\n",
      " 350/1000 [ 35%] ██████████                     ETA: 76s | Loss: 7.219=\n",
      " 400/1000 [ 40%] ████████████                   ETA: 61s | Loss: 6.455=\n",
      " 450/1000 [ 45%] █████████████                  ETA: 50s | Loss: 7.353=\n",
      " 500/1000 [ 50%] ███████████████                ETA: 41s | Loss: 7.156=\n",
      " 550/1000 [ 55%] ████████████████               ETA: 34s | Loss: 7.056=\n",
      " 600/1000 [ 60%] ██████████████████             ETA: 27s | Loss: 7.218=\n",
      " 650/1000 [ 65%] ███████████████████            ETA: 22s | Loss: 7.078=\n",
      " 700/1000 [ 70%] █████████████████████          ETA: 18s | Loss: 7.224=\n",
      " 750/1000 [ 75%] ██████████████████████         ETA: 14s | Loss: 6.957=\n",
      " 800/1000 [ 80%] ████████████████████████       ETA: 10s | Loss: 7.220=\n",
      " 850/1000 [ 85%] █████████████████████████      ETA: 7s | Loss: 6.634 =\n",
      " 900/1000 [ 90%] ███████████████████████████    ETA: 4s | Loss: 7.147=\n",
      " 950/1000 [ 95%] ████████████████████████████   ETA: 2s | Loss: 6.999=\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 43s | Loss: 7.175\n",
      "0.62358296\n"
     ]
    }
   ],
   "source": [
    "data = [1,1,1,1,1,1,0,0,0,0]\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"input\"):\n",
    "        X = tf.constant(data, dtype=tf.int32)\n",
    "    with tf.name_scope(\"model\"):\n",
    "        theta = mod.Beta(10.0,10.0)\n",
    "        model = mod.Bernoulli(probs=tf.ones(len(data))*theta)\n",
    "    with tf.name_scope(\"posterior\"):\n",
    "        alpha = tf.Variable(15.0)\n",
    "        beta = tf.Variable(15.0)\n",
    "        qtheta = mod.Beta(alpha, beta)\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    n_iter = 1000\n",
    "    loss = np.zeros(n_iter)\n",
    "    inference = ed.KLqp(\n",
    "        {\n",
    "            theta: qtheta,\n",
    "        }, data={\n",
    "            model: X,\n",
    "        })\n",
    "    optimizer = tf.train.AdamOptimizer(0.0005,0.90, 0.999)\n",
    "    inference.initialize(\n",
    "        n_iter=n_iter, optimizer=optimizer, n_samples=100, n_print=n_iter // 20)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(inference.n_iter):\n",
    "        info_dict = inference.update()\n",
    "        inference.print_progress(info_dict)\n",
    "        loss[i] = info_dict[\"loss\"]\n",
    "        if i % inference.n_print == 0:\n",
    "            print(\"=\")\n",
    "\n",
    "    inference.finalize()\n",
    "    alpha_result = alpha.eval()\n",
    "    beta_result = beta.eval()\n",
    "    theta_sample = qtheta.eval()\n",
    "# inference.run(n_iter=10000, optimizer=optimizer)\n",
    "\n",
    "print(np.mean(theta_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_hat=16\n",
    "b1_hat= 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Samples = 1000\n",
    "q=[] \n",
    "for i in range(Samples):\n",
    "    q.append(pyro.sample(\"latent_fairness\", dist.Beta(alpha_result, beta_result)).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pos = stats.beta(a1_hat,b1_hat).pdf(q)\n",
    "q=stats.beta(alpha_result,beta_result).pdf(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ratio = []\n",
    "ind = 0\n",
    "for i in Pos:\n",
    "    Ratio.append(i/q[ind])\n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:-0.15585779781646708\n"
     ]
    }
   ],
   "source": [
    "print('K:'+str(psisloo(1*np.array(Ratio))[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:0.32132535923141087\n"
     ]
    }
   ],
   "source": [
    "print('K:'+str(psisloo(-1*np.array(Ratio))[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = beta_result / (alpha_result * (1.0 + alpha_result + beta_result))\n",
    "inferred_std = np.mean(theta_sample) * math.sqrt(factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$$0.11186274366119489$$"
      ],
      "text/plain": [
       "0.11186274366119489"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psisloo(log_lik, **kwargs):\n",
    "    r\"\"\"PSIS leave-one-out log predictive densities.\n",
    "\n",
    "    Computes the log predictive densities given posterior samples of the log\n",
    "    likelihood terms :math:`p(y_i|\\theta^s)` in input parameter `log_lik`.\n",
    "    Returns a sum of the leave-one-out log predictive densities `loo`,\n",
    "    individual leave-one-out log predictive density terms `loos` and an estimate\n",
    "    of Pareto tail indeces `ks`. The estimates are unreliable if tail index\n",
    "    ``k > 0.7`` (see more in the references listed in the module docstring).\n",
    "\n",
    "    Additional keyword arguments are passed to the :meth:`psislw()` function\n",
    "    (see the corresponding documentation).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    log_lik : ndarray\n",
    "        Array of size n x m containing n posterior samples of the log likelihood\n",
    "        terms :math:`p(y_i|\\theta^s)`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loo : scalar\n",
    "        sum of the leave-one-out log predictive densities\n",
    "\n",
    "    loos : ndarray\n",
    "        individual leave-one-out log predictive density terms\n",
    "\n",
    "    ks : ndarray\n",
    "        estimated Pareto tail indeces\n",
    "\n",
    "    \"\"\"\n",
    "    # ensure overwrite flag in passed arguments\n",
    "    kwargs['overwrite_lw'] = True\n",
    "    # log raw weights from log_lik\n",
    "    lw = -log_lik\n",
    "    # compute Pareto smoothed log weights given raw log weights\n",
    "    lw, ks = psislw(lw, **kwargs)\n",
    "    # compute\n",
    "    lw += log_lik\n",
    "    loos = sumlogs(lw, axis=0)\n",
    "    loo = loos.sum()\n",
    "    #print('Ks'+str(ks))\n",
    "    return loo, loos, ks\n",
    "\n",
    "def psislw(lw, Reff=1.0, overwrite_lw=False):\n",
    "    \"\"\"Pareto smoothed importance sampling (PSIS).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lw : ndarray\n",
    "        Array of size n x m containing m sets of n log weights. It is also\n",
    "        possible to provide one dimensional array of length n.\n",
    "\n",
    "    Reff : scalar, optional\n",
    "        relative MCMC efficiency ``N_eff / N``\n",
    "\n",
    "    overwrite_lw : bool, optional\n",
    "        If True, the input array `lw` is smoothed in-place, assuming the array\n",
    "        is F-contiguous. By default, a new array is allocated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lw_out : ndarray\n",
    "        smoothed log weights\n",
    "    kss : ndarray\n",
    "        Pareto tail indices\n",
    "\n",
    "    \"\"\"\n",
    "    if lw.ndim == 2:\n",
    "        n, m = lw.shape\n",
    "    elif lw.ndim == 1:\n",
    "        n = len(lw)\n",
    "        m = 1\n",
    "    else:\n",
    "        raise ValueError(\"Argument `lw` must be 1 or 2 dimensional.\")\n",
    "    if n <= 1:\n",
    "        raise ValueError(\"More than one log-weight needed.\")\n",
    "\n",
    "    if overwrite_lw and lw.flags.f_contiguous:\n",
    "        # in-place operation\n",
    "        lw_out = lw\n",
    "    else:\n",
    "        # allocate new array for output\n",
    "        lw_out = np.copy(lw, order='F')\n",
    "\n",
    "    # allocate output array for kss\n",
    "    kss = np.empty(m)\n",
    "    #print(kss)\n",
    "\n",
    "    # precalculate constants\n",
    "    cutoff_ind = - int(np.ceil(min(0.2 * n, 3 * np.sqrt(n / Reff)))) - 1\n",
    "    cutoffmin = np.log(np.finfo(float).tiny)\n",
    "    logn = np.log(n)\n",
    "    k_min = 1/3\n",
    "\n",
    "    # loop over sets of log weights\n",
    "    for i, x in enumerate(lw_out.T if lw_out.ndim == 2 else lw_out[None, :]):\n",
    "        # improve numerical accuracy\n",
    "        x -= np.max(x)\n",
    "        # sort the array\n",
    "        x_sort_ind = np.argsort(x)\n",
    "        # divide log weights into body and right tail\n",
    "        xcutoff = max(\n",
    "            x[x_sort_ind[cutoff_ind]],\n",
    "            cutoffmin\n",
    "        )\n",
    "        expxcutoff = np.exp(xcutoff)\n",
    "        tailinds, = np.where(x > xcutoff)\n",
    "        x2 = x[tailinds]\n",
    "        n2 = len(x2)\n",
    "        if n2 <= 4:\n",
    "            # not enough tail samples for gpdfitnew\n",
    "            k = np.inf\n",
    "        else:\n",
    "            # order of tail samples\n",
    "            x2si = np.argsort(x2)\n",
    "            # fit generalized Pareto distribution to the right tail samples\n",
    "            np.exp(x2, out=x2)\n",
    "            x2 -= expxcutoff\n",
    "            k, sigma = gpdfitnew(x2, sort=x2si)\n",
    "        if k >= k_min and not np.isinf(k):\n",
    "            # no smoothing if short tail or GPD fit failed\n",
    "            # compute ordered statistic for the fit\n",
    "            sti = np.arange(0.5, n2)\n",
    "            sti /= n2\n",
    "            qq = gpinv(sti, k, sigma)\n",
    "            qq += expxcutoff\n",
    "            np.log(qq, out=qq)\n",
    "            # place the smoothed tail into the output array\n",
    "            x[tailinds[x2si]] = qq\n",
    "            # truncate smoothed values to the largest raw weight 0\n",
    "            x[x > 0] = 0\n",
    "        # renormalize weights\n",
    "        x -= sumlogs(x)\n",
    "        # store tail index k\n",
    "        kss[i] = k\n",
    "        #print(k)\n",
    "\n",
    "    # If the provided input array is one dimensional, return kss as scalar.\n",
    "    if lw_out.ndim == 1:\n",
    "        kss = kss[0]\n",
    "\n",
    "    return lw_out, kss\n",
    "\n",
    "def sumlogs(x, axis=None, out=None):\n",
    "    \"\"\"Sum of vector where numbers are represented by their logarithms.\n",
    "\n",
    "    Calculates ``np.log(np.sum(np.exp(x), axis=axis))`` in such a fashion that\n",
    "    it works even when elements have large magnitude.\n",
    "\n",
    "    \"\"\"\n",
    "    maxx = x.max(axis=axis, keepdims=True)\n",
    "    xnorm = x - maxx\n",
    "    np.exp(xnorm, out=xnorm)\n",
    "    out = np.sum(xnorm, axis=axis, out=out)\n",
    "    if isinstance(out, np.ndarray):\n",
    "        np.log(out, out=out)\n",
    "    else:\n",
    "        out = np.log(out)\n",
    "    out += np.squeeze(maxx)\n",
    "    return out\n",
    "def gpdfitnew(x, sort=True, sort_in_place=False, return_quadrature=False):\n",
    "    \"\"\"Estimate the paramaters for the Generalized Pareto Distribution (GPD)\n",
    "\n",
    "    Returns empirical Bayes estimate for the parameters of the two-parameter\n",
    "    generalized Parato distribution given the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        One dimensional data array\n",
    "\n",
    "    sort : bool or ndarray, optional\n",
    "        If known in advance, one can provide an array of indices that would\n",
    "        sort the input array `x`. If the input array is already sorted, provide\n",
    "        False. If True (default behaviour), the array is sorted internally.\n",
    "\n",
    "    sort_in_place : bool, optional\n",
    "        If `sort` is True and `sort_in_place` is True, the array is sorted\n",
    "        in-place (False by default).\n",
    "\n",
    "    return_quadrature : bool, optional\n",
    "        If True, quadrature points and weight `ks` and `w` of the marginal posterior distribution of k are also calculated and returned. False by\n",
    "        default.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    k, sigma : float\n",
    "        estimated parameter values\n",
    "\n",
    "    ks, w : ndarray\n",
    "        Quadrature points and weights of the marginal posterior distribution\n",
    "        of `k`. Returned only if `return_quadrature` is True.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function returns a negative of Zhang and Stephens's k, because it is\n",
    "    more common parameterisation.\n",
    "\n",
    "    \"\"\"\n",
    "    if x.ndim != 1 or len(x) <= 1:\n",
    "        raise ValueError(\"Invalid input array.\")\n",
    "\n",
    "    # check if x should be sorted\n",
    "    if sort is True:\n",
    "        if sort_in_place:\n",
    "            x.sort()\n",
    "            xsorted = True\n",
    "        else:\n",
    "            sort = np.argsort(x)\n",
    "            xsorted = False\n",
    "    elif sort is False:\n",
    "        xsorted = True\n",
    "    else:\n",
    "        xsorted = False\n",
    "\n",
    "    n = len(x)\n",
    "    PRIOR = 3\n",
    "    m = 30 + int(np.sqrt(n))\n",
    "\n",
    "    bs = np.arange(1, m + 1, dtype=float)\n",
    "    bs -= 0.5\n",
    "    np.divide(m, bs, out=bs)\n",
    "    np.sqrt(bs, out=bs)\n",
    "    np.subtract(1, bs, out=bs)\n",
    "    if xsorted:\n",
    "        bs /= PRIOR * x[int(n/4 + 0.5) - 1]\n",
    "        bs += 1 / x[-1]\n",
    "    else:\n",
    "        bs /= PRIOR * x[sort[int(n/4 + 0.5) - 1]]\n",
    "        bs += 1 / x[sort[-1]]\n",
    "\n",
    "    ks = np.negative(bs)\n",
    "    temp = ks[:,None] * x\n",
    "    np.log1p(temp, out=temp)\n",
    "    np.mean(temp, axis=1, out=ks)\n",
    "\n",
    "    L = bs / ks\n",
    "    np.negative(L, out=L)\n",
    "    np.log(L, out=L)\n",
    "    L -= ks\n",
    "    L -= 1\n",
    "    L *= n\n",
    "\n",
    "    temp = L - L[:,None]\n",
    "    np.exp(temp, out=temp)\n",
    "    w = np.sum(temp, axis=1)\n",
    "    np.divide(1, w, out=w)\n",
    "\n",
    "    # remove negligible weights\n",
    "    dii = w >= 10 * np.finfo(float).eps\n",
    "    if not np.all(dii):\n",
    "        w = w[dii]\n",
    "        bs = bs[dii]\n",
    "    # normalise w\n",
    "    w /= w.sum()\n",
    "\n",
    "    # posterior mean for b\n",
    "    b = np.sum(bs * w)\n",
    "    # Estimate for k, note that we return a negative of Zhang and\n",
    "    # Stephens's k, because it is more common parameterisation.\n",
    "    temp = (-b) * x\n",
    "    np.log1p(temp, out=temp)\n",
    "    k = np.mean(temp)\n",
    "    if return_quadrature:\n",
    "        np.negative(x, out=temp)\n",
    "        temp = bs[:, None] * temp\n",
    "        np.log1p(temp, out=temp)\n",
    "        ks = np.mean(temp, axis=1)\n",
    "    # estimate for sigma\n",
    "    sigma = -k / b * n / (n - 0)\n",
    "    # weakly informative prior for k\n",
    "    a = 10\n",
    "    k = k * n / (n+a) + a * 0.5 / (n+a)\n",
    "    if return_quadrature:\n",
    "        ks *= n / (n+a)\n",
    "        ks += a * 0.5 / (n+a)\n",
    "\n",
    "    if return_quadrature:\n",
    "        #print('K:'+str(k))\n",
    "        return k, sigma, ks, w\n",
    "    else:\n",
    "        #print('K:'+str(k))\n",
    "        return k, sigma\n",
    "\n",
    "def gpinv(p, k, sigma):\n",
    "    \"\"\"Inverse Generalised Pareto distribution function.\"\"\"\n",
    "    x = np.empty(p.shape)\n",
    "    x.fill(np.nan)\n",
    "    if sigma <= 0:\n",
    "        return x\n",
    "    ok = (p > 0) & (p < 1)\n",
    "    if np.all(ok):\n",
    "        if np.abs(k) < np.finfo(float).eps:\n",
    "            np.negative(p, out=x)\n",
    "            np.log1p(x, out=x)\n",
    "            np.negative(x, out=x)\n",
    "        else:\n",
    "            np.negative(p, out=x)\n",
    "            np.log1p(x, out=x)\n",
    "            x *= -k\n",
    "            np.expm1(x, out=x)\n",
    "            x /= k\n",
    "        x *= sigma\n",
    "    else:\n",
    "        if np.abs(k) < np.finfo(float).eps:\n",
    "            # x[ok] = - np.log1p(-p[ok])\n",
    "            temp = p[ok]\n",
    "            np.negative(temp, out=temp)\n",
    "            np.log1p(temp, out=temp)\n",
    "            np.negative(temp, out=temp)\n",
    "            x[ok] = temp\n",
    "        else:\n",
    "            # x[ok] = np.expm1(-k * np.log1p(-p[ok])) / k\n",
    "            temp = p[ok]\n",
    "            np.negative(temp, out=temp)\n",
    "            np.log1p(temp, out=temp)\n",
    "            temp *= -k\n",
    "            np.expm1(temp, out=temp)\n",
    "            temp /= k\n",
    "            x[ok] = temp\n",
    "        x *= sigma\n",
    "        x[p == 0] = 0\n",
    "        if k >= 0:\n",
    "            x[p == 1] = np.inf\n",
    "        else:\n",
    "            x[p == 1] = -sigma / k\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(P,Q):\n",
    "    epsilon = 0.00001\n",
    "\n",
    "    # You may want to instead make copies to avoid changing the np arrays.\n",
    "    P = P+epsilon\n",
    "    Q = Q+epsilon\n",
    "\n",
    "    divergence = np.sum(P*np.log2(P/Q))\n",
    "    return divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
